{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Data Engineering Capstone Project\n",
    "### Authored by: Patrick Groover\n",
    "##### Authored on: December 2020\n",
    "\n",
    "##### Sources Include: Personal Project Work, Answers Provided within the \"Mentor Help\" Responses for this project within the Udacity Learning Portal, and Code Mentoring Sessions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Project Summary\n",
    "\n",
    "This project was designed to demonstrate the skills and approaches learned within the Data Engineering Class that allow for the planning, analysis, visualization, and wrangling of data to deliver an automated pipeline of information for analytics purposes. To demonstrate these learnings, students were asked to develop a process that would incorporate 4 distinct sets of data, with at least one being Big Data (defined as a source over 1 Million Rows), to support a targeted analysis that mirrors a real world purpose. To demonstrate knowledge acquired within the course, this project leveraged the following approach to produce the working deliverables:\n",
    "\n",
    "    Step 1: Scope the Project and Gather Data\n",
    "    Step 2: Explore and Assess the Data\n",
    "    Step 3: Define the Data Model\n",
    "    Step 4: Run ETL to Model the Data\n",
    "    Step 5: Complete Project Write Up\n",
    "    \n",
    "This project leveraged data transformations and handling for multiple data types, including csv, json, and parquet files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### File Setup & Process for Launching Project\n",
    "\n",
    "#### Run ETL using EMR Cluster\n",
    "\n",
    "    A) Set Target Directory for ETL Files\n",
    "    B) Access \"dl.cfg\" file and add AWS Credentials, then update Target Directory as \"OUTPUT_BUCKET=<Target Directory>\"\n",
    "    C) Launch EMR Cluster\n",
    "    D) Load \"capstone_etl.py\" and \"dl.cfg\" files to EMR Cluster\n",
    "    E) Run \"capstone_etl.py\"\n",
    "    \n",
    "#### Run Airflow to Create & Validate Analytics Fact & Dimension Tables within Redshift\n",
    "\n",
    "#### Run Airflow to Create & Validate Analytics Fact & Dimension Tables within Redshift\n",
    "\n",
    "    A) Load files from \"Capstone Airflow Files\" Directory into Airflow matching the Directory Structure (see Step 4.1.2)\n",
    "    B) Open the file \"capstone_dag.py\" and update the \"data_path\" to match the Target Directory within the ETL Process for the following dag operator statements:\n",
    "        - import_i94_visit_details_fact\n",
    "        - import_state_demographics_dim\n",
    "        - import_ethnicity_by_state\n",
    "        - import_temperatures_dim\n",
    "        - import_us_airports_size_dim\n",
    "    C) Launch Airflow & Add AWS Credentials and Redshift Credentials\n",
    "    D) Run \"capstone_dag\" in Airflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Step 1: Scope the Project and Gather Data\n",
    "\n",
    "#### Sample Scenario that directed project outcomes:\n",
    "\n",
    "Travel & Immigration Data to the United States can help city and state officials plan for tourism, naturalization, business, and educational needs. The seasonal variance of such activites can lead to staffing, security, and support issues that make it challenging to address the volume and needs of visitors. To help with planning, the U.S. Travel & Immigration Department has requested that a database of visitor information be created that helps identify patterns of ingress including a look at orgins, destinations, visa type, demographics of visitor, demographics of destination state, access to a major airport, as well as temperatures of origin and destination city. The datalake will serve as the foundation for adding additional data sources to analytics research in the future."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 1.1 Main Data Sources for the Project\n",
    "\n",
    "- **I94 Immigration Data:** This data comes from the US National Tourism and Trade Office and is a listing of visits to the United States.\n",
    "\n",
    "> Link to Immigration Data Source\n",
    "> https://travel.trade.gov/research/reports/i94/historical/2016.html\n",
    "\n",
    "- **World Temperature Data:** This dataset came from Kaggle that will correlate temperatures of US Destination and Country of Origen by Month.\n",
    "\n",
    "> Link to World Temperature Data Source      \n",
    "> https://www.kaggle.com/berkeleyearth/climate-change-earth-surface-temperature-data\n",
    "\n",
    "- **U.S. City Demographic Data:** This data comes from OpenSoft and will be used to correlate US Destination Demographics by State.\n",
    "\n",
    "> Link to U.S. City Demographic Data      \n",
    "> https://public.opendatasoft.com/explore/dataset/us-cities-demographics/export/\n",
    "\n",
    "- **Airport Code Table:** This is a simple table of airport codes and corresponding cities the will be used to correlate Airport Size to US Destination.\n",
    "\n",
    "> Link to Airport Code Table      \n",
    "> https://datahub.io/core/airport-codes#data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 1.2 Secondary Data Sources for Creating Analytics Tables\n",
    "\n",
    "These files are found in the publicly accessible S3 Bucket: \"pg-001-1st-test-bucket\"\n",
    "\n",
    "And Located within the following directory path: \"capstone/analytics_keys/\n",
    "\n",
    "- **Location Codes:** A table of aggregate Country & City Codes that related to the Location Fields within the i94 Immigration Data.\n",
    "\n",
    "> File Name for Location Codes: \"location_codes.csv\"\n",
    "\n",
    "- **Travel Mode Codes:** A table of travel codes that denote the transportation type used to reach the United States.\n",
    "\n",
    "> File Name for Travel Mode Codes: \"travel_mode_code.csv\"\n",
    "\n",
    "- **Travel Purpose Codes:** A table of travel purpose codes that denotes the main reason for visiting the United States.\n",
    "\n",
    "> File Name for Travel Purpose Codes: \"travel_purpose.csv\"\n",
    "\n",
    "- **Visa Type Codes:** A table of Visa Type Codes that denotes the classification of the visit to the United States.\n",
    "\n",
    "> File Name for Visa Type Codes: \"visa_type_codes.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 1.3 Tools Used to Develop Framework and Pipeline\n",
    "\n",
    "    A) Analysis of file sources & structures through Pyspark within Jupyter Notebooks\n",
    "    B) Creation of a Star Schema and Data Processing Flow Diagram using Lucid Chart\n",
    "    C) Development of ETL Process using Jupyter Notebooks, tested on a subset of Data within a Pyspark Dataframe\n",
    "    D) Execution of ETL Process through an EMR Cluster to produce Dimension & Fact Tables that were loaded to the targeted Amazon S3 Bucket\n",
    "    E) Development of a Pipeline Process for ingesting & updating the Redshift Analytics Database through Airflow\n",
    "    F) Execution of the Airflow Process with Data Validation to produce a working analytics data set\n",
    "    G) Review of delivered information within Redshift for a final validation of delivered results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 1.4.1 Targeted Solution\n",
    "\n",
    "The ultimate outcome of the project will be a star schema analytics database that is easy for analytics users to query. The database will relate through a central Fact Table of i94 Visit Details and will need to link to Dimension Tables using record keys to make analytics queries simple to non-engineering resources and ultimately save any future need for transforming information in order to link disparate data sets.\n",
    "\n",
    "The following steps detail the data discovery that was used to produce the final Star Schema Analytics Database that is seen here:\n",
    "![i94 Visit Analytics Star Schema](http://na-sjdemo1.marketo.com/rs/786-GZR-035/images/Capstone_i94_Analytics_Star_Schema.png \"i94 Visit Analytics Star Schema\")\n",
    "\n",
    "> External Link to View Star Schema       \n",
    "> http://na-sjdemo1.marketo.com/rs/786-GZR-035/images/Capstone_i94_Analytics_Star_Schema.png\n",
    "\n",
    "#### 1.4.2 Data Model\n",
    "\n",
    "To deliver a simple-to-use Analytics Repository that is accessible through Amazon Redshift, a Relational Data Model was selected that uses primary keys that link to foreign keys within the Star Schema. To produce this schema, a series of joins were used within the ETL process to make it easy for Analytics Users to work mainly from primary keys and occassionally a primary key plus a secondary key. The i94_travel_details_fact table houses the majority of the keys that are then easily related to the dimension tables.\n",
    "\n",
    "This data model was selected for the ease of use to end users, particularly after the development of the simplified primary key structure as well as its extensibility over time to append additional data sources leverage established keys, paricularly the location keys. For example, analytics users may want to add insights around weath and economics based on country, state or city over time - using the same process, data engineers could produce an \"economics\" dimension table that links to the main i94_visit_details_fact table.\n",
    "\n",
    "**Primary Keys include:**\n",
    "\n",
    "| Table                            |      Field Name      |  Data Type  |                  Description                 |\n",
    "| :---                             |        :----:        |    :----:   |                                         ---: |\n",
    "| i94_visit_details_fact           |        i94rec        |   Integer   |  Visit Details Record ID                     |\n",
    "| temperatures_dim                 |    temperature_id    |   Bigint    |  Temperatures Record ID                      |\n",
    "| state_demographics_dim           |    i94_state_code    |   Varchar   |  Location Code ID for State, i.e. \"GA\"       |\n",
    "| ethnicity_by_state_dim           |    i94_state_code    |   Varchar   |  Location Code ID for State, i.e. \"GA\"       |\n",
    "| us_airport_size_dim              |    i94_port_code     |   Varchar   |  Location Code ID for City, i.e. \"ORL\"       |\n",
    "| location_codes_dim               |   location_code_id   |   Integer   |  Master Location Code                        |\n",
    "| travel_mode_dim                  |   travel_mode_code   |   Varchar   |  Master Travel Mode Code                     |\n",
    "| travel_purpose_dim               |  travel_purpose_code |   Varchar   |  Master Travel Purpose Code                  |\n",
    "| visa_type_codes_dim              |       visa_code      |   Varchar   |  Master Visa Type Code                       |\n",
    "\n",
    "With Location Keys being used for a significant portion of the analytics associations, the location_codes_dim is particularly important for relating location keys to location data, such as Country, State, and City.\n",
    "\n",
    "**Location Keys Include:**\n",
    "\n",
    "| Field Name                |  Data Type   |    Key Type    |                      Description                      |\n",
    "| :---                      |    :----:    |     :----:     |                                                  ---: |\n",
    "| location_code_id          |    Integer   |   Primary Key  |  Master Location Code                                 |\n",
    "| country_code              |    Varchar   |   Foreign Key  |  Country Location Code                                |\n",
    "| country                   |    Varchar   |       n/a      |  Full Name of Country                                 |\n",
    "| state_code                |    Varchar   |   Foreign Key  |  State Location Code                                  |\n",
    "| city                      |    Varchar   |       n/a      |  City Location Name                                   |\n",
    "| state                     |    Varchar   |       n/a      |  Full State Name                                      |\n",
    "\n",
    "For development and processing of the ETL files, a No-SQL data model was used to process Big Data through Pyspark. This data model was selected for it's ease of use when analyzing, wrangling and processing disparate data from different sources. Use of \"Schema-on-read\" made it easier to develop and link together resources that varied widely in their formating and structure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 1.5 Gather Data \n",
    "\n",
    "To support the development of the Analytics Star Schema initial Data Sources & Analytics Data Tables were loaded to S3 Buckets within \"pg-001-1st-test-bucket\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Step 2: Explore and Assess the Data\n",
    "\n",
    "To develop the Fact & Dimension Tables that would support the final Analytics Data Set and Pipeline, it was necessary to use an itterative approach to analyzing existing dataset, then adapting the scope of analytics that would be available due to data formats and gaps within the data sets.\n",
    "\n",
    "The process leveraged the following Pyspark Imports to analyze and adjust the flow for integrating data sets into a final, workable, and connected pipeline.\n",
    "\n",
    "#### Explore the Data \n",
    "Identify data quality issues, like missing values, duplicate data, etc.\n",
    "\n",
    "#### Cleaning Steps\n",
    "Document steps necessary to clean the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### 2.0.1 Import Code Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Imports and installs\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext as sc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython.display import display\n",
    "from zipfile import ZipFile\n",
    "import os\n",
    "import io\n",
    "from pyspark.sql.functions import isnan, count, when, col, desc, udf, col, sort_array, asc, avg, substring\n",
    "from pyspark.sql.functions import sum as Fsum\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import IntegerType"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 2.1.1 i94 Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Read in the i94 data\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.config(\"spark.jars.packages\",\"saurfang:spark-sas7bdat:2.0.0-s_2.11\").enableHiveSupport().getOrCreate()\n",
    "fpath = '../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat'\n",
    "i94_df = spark.read\\\n",
    "    .option(\"header\", \"true\")\\\n",
    "    .option(\"inferSchema\", \"true\")\\\n",
    "    .format('com.github.saurfang.sas.spark').load(fpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- cicid: double (nullable = true)\n",
      " |-- i94yr: double (nullable = true)\n",
      " |-- i94mon: double (nullable = true)\n",
      " |-- i94cit: double (nullable = true)\n",
      " |-- i94res: double (nullable = true)\n",
      " |-- i94port: string (nullable = true)\n",
      " |-- arrdate: double (nullable = true)\n",
      " |-- i94mode: double (nullable = true)\n",
      " |-- i94addr: string (nullable = true)\n",
      " |-- depdate: double (nullable = true)\n",
      " |-- i94bir: double (nullable = true)\n",
      " |-- i94visa: double (nullable = true)\n",
      " |-- count: double (nullable = true)\n",
      " |-- dtadfile: string (nullable = true)\n",
      " |-- visapost: string (nullable = true)\n",
      " |-- occup: string (nullable = true)\n",
      " |-- entdepa: string (nullable = true)\n",
      " |-- entdepd: string (nullable = true)\n",
      " |-- entdepu: string (nullable = true)\n",
      " |-- matflag: string (nullable = true)\n",
      " |-- biryear: double (nullable = true)\n",
      " |-- dtaddto: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- insnum: string (nullable = true)\n",
      " |-- airline: string (nullable = true)\n",
      " |-- admnum: double (nullable = true)\n",
      " |-- fltno: string (nullable = true)\n",
      " |-- visatype: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Visualize the i94 data\n",
    "i94_df.head()\n",
    "i94_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cicid</th>\n",
       "      <th>i94yr</th>\n",
       "      <th>i94mon</th>\n",
       "      <th>i94cit</th>\n",
       "      <th>i94res</th>\n",
       "      <th>i94port</th>\n",
       "      <th>arrdate</th>\n",
       "      <th>i94mode</th>\n",
       "      <th>i94addr</th>\n",
       "      <th>depdate</th>\n",
       "      <th>i94bir</th>\n",
       "      <th>i94visa</th>\n",
       "      <th>count</th>\n",
       "      <th>dtadfile</th>\n",
       "      <th>visapost</th>\n",
       "      <th>occup</th>\n",
       "      <th>entdepa</th>\n",
       "      <th>entdepd</th>\n",
       "      <th>entdepu</th>\n",
       "      <th>matflag</th>\n",
       "      <th>biryear</th>\n",
       "      <th>dtaddto</th>\n",
       "      <th>gender</th>\n",
       "      <th>insnum</th>\n",
       "      <th>airline</th>\n",
       "      <th>admnum</th>\n",
       "      <th>fltno</th>\n",
       "      <th>visatype</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>692.0</td>\n",
       "      <td>692.0</td>\n",
       "      <td>XXX</td>\n",
       "      <td>20573.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>37.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>T</td>\n",
       "      <td>None</td>\n",
       "      <td>U</td>\n",
       "      <td>None</td>\n",
       "      <td>1979.0</td>\n",
       "      <td>10282016</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>1.897628e+09</td>\n",
       "      <td>None</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>254.0</td>\n",
       "      <td>276.0</td>\n",
       "      <td>ATL</td>\n",
       "      <td>20551.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>AL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>25.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20130811</td>\n",
       "      <td>SEO</td>\n",
       "      <td>None</td>\n",
       "      <td>G</td>\n",
       "      <td>None</td>\n",
       "      <td>Y</td>\n",
       "      <td>None</td>\n",
       "      <td>1991.0</td>\n",
       "      <td>D/S</td>\n",
       "      <td>M</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>3.736796e+09</td>\n",
       "      <td>00296</td>\n",
       "      <td>F1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>WAS</td>\n",
       "      <td>20545.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>MI</td>\n",
       "      <td>20691.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20160401</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>T</td>\n",
       "      <td>O</td>\n",
       "      <td>None</td>\n",
       "      <td>M</td>\n",
       "      <td>1961.0</td>\n",
       "      <td>09302016</td>\n",
       "      <td>M</td>\n",
       "      <td>None</td>\n",
       "      <td>OS</td>\n",
       "      <td>6.666432e+08</td>\n",
       "      <td>93</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>NYC</td>\n",
       "      <td>20545.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>MA</td>\n",
       "      <td>20567.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20160401</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>None</td>\n",
       "      <td>M</td>\n",
       "      <td>1988.0</td>\n",
       "      <td>09302016</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>AA</td>\n",
       "      <td>9.246846e+10</td>\n",
       "      <td>00199</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>NYC</td>\n",
       "      <td>20545.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>MA</td>\n",
       "      <td>20567.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20160401</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>None</td>\n",
       "      <td>M</td>\n",
       "      <td>2012.0</td>\n",
       "      <td>09302016</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>AA</td>\n",
       "      <td>9.246846e+10</td>\n",
       "      <td>00199</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cicid   i94yr  i94mon  i94cit  i94res i94port  arrdate  i94mode i94addr  \\\n",
       "0    6.0  2016.0     4.0   692.0   692.0     XXX  20573.0      NaN    None   \n",
       "1    7.0  2016.0     4.0   254.0   276.0     ATL  20551.0      1.0      AL   \n",
       "2   15.0  2016.0     4.0   101.0   101.0     WAS  20545.0      1.0      MI   \n",
       "3   16.0  2016.0     4.0   101.0   101.0     NYC  20545.0      1.0      MA   \n",
       "4   17.0  2016.0     4.0   101.0   101.0     NYC  20545.0      1.0      MA   \n",
       "\n",
       "   depdate  i94bir  i94visa  count  dtadfile visapost occup entdepa entdepd  \\\n",
       "0      NaN    37.0      2.0    1.0      None     None  None       T    None   \n",
       "1      NaN    25.0      3.0    1.0  20130811      SEO  None       G    None   \n",
       "2  20691.0    55.0      2.0    1.0  20160401     None  None       T       O   \n",
       "3  20567.0    28.0      2.0    1.0  20160401     None  None       O       O   \n",
       "4  20567.0     4.0      2.0    1.0  20160401     None  None       O       O   \n",
       "\n",
       "  entdepu matflag  biryear   dtaddto gender insnum airline        admnum  \\\n",
       "0       U    None   1979.0  10282016   None   None    None  1.897628e+09   \n",
       "1       Y    None   1991.0       D/S      M   None    None  3.736796e+09   \n",
       "2    None       M   1961.0  09302016      M   None      OS  6.666432e+08   \n",
       "3    None       M   1988.0  09302016   None   None      AA  9.246846e+10   \n",
       "4    None       M   2012.0  09302016   None   None      AA  9.246846e+10   \n",
       "\n",
       "   fltno visatype  \n",
       "0   None       B2  \n",
       "1  00296       F1  \n",
       "2     93       B2  \n",
       "3  00199       B2  \n",
       "4  00199       B2  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display in Pandas to View Column Headers & Data Types\n",
    "pd.set_option('display.max_columns', 200)\n",
    "i94_df.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### 2.1.2 Conclusions of i94 Visits Data Analysis\n",
    "    A) Column headers will need to be transformed to match more user-friendly naming schemas\n",
    "    B) Addition of temperature keys for i94 Port & Visitor Residence will need to be appended"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### 2.1.3 i94 Visits Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Create i94_fact_select Dataframe\n",
    "i94_fact_select = i94_df.select('cicid','i94yr','i94mon','i94cit','i94res','i94port','arrdate','i94mode','i94addr','depdate','i94bir','i94visa','count','biryear','gender','visatype')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Prepare Fact Select Dataframe for final additions by adding column headers\n",
    "\n",
    "i94_pre_fact_table = i94_fact_select\\\n",
    "    .withColumnRenamed('cicid','i94rec')\\\n",
    "    .withColumnRenamed('i94yr','i94_year')\\\n",
    "    .withColumnRenamed('i94mon','i94_month')\\\n",
    "    .withColumnRenamed('i94cit','i94_citizenship')\\\n",
    "    .withColumnRenamed('i94res','i94_residence')\\\n",
    "    .withColumnRenamed('i94port','i94_port_of_entry')\\\n",
    "    .withColumnRenamed('arrdate','arrival_date')\\\n",
    "    .withColumnRenamed('i94mode','arrival_mode')\\\n",
    "    .withColumnRenamed('i94addr','arrival_state')\\\n",
    "    .withColumnRenamed('depdate','departure_date')\\\n",
    "    .withColumnRenamed('i94bir','i94_age')\\\n",
    "    .withColumnRenamed('i94visa','travel_purpose')\\\n",
    "    .withColumnRenamed('count','count')\\\n",
    "    .withColumnRenamed('biryear','birth_year')\\\n",
    "    .withColumnRenamed('gender','gender')\\\n",
    "    .withColumnRenamed('visatype','visa_type')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3096313"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Visualize the Fact Select Dataframe and Validate Count of Rows\n",
    "i94_pre_fact_table.limit(5).toPandas()\n",
    "i94_pre_fact_table.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Create Temptable for Creation of final Fact Table\n",
    "i94_pre_fact_table.createOrReplaceTempView(\"i94_pre_fact_temptable\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 2.2.1 Location Codes Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Read in Location Codes Data\n",
    "location_file = 'raw_data/location_codes.csv'\n",
    "location_codes_df = spark.read\\\n",
    "    .option(\"header\", \"true\")\\\n",
    "    .option(\"inferSchema\", \"true\")\\\n",
    "    .csv(location_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- location_code_id: string (nullable = true)\n",
      " |-- country_code: integer (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- state_code: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "818"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Visualize Location Data & Count Rows\n",
    "location_codes_df.head()\n",
    "location_codes_df.printSchema()\n",
    "location_codes_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>location_code_id</th>\n",
       "      <th>country_code</th>\n",
       "      <th>country</th>\n",
       "      <th>state_code</th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>lc_001</td>\n",
       "      <td>582</td>\n",
       "      <td>Mexico</td>\n",
       "      <td>non-us-no-state-code</td>\n",
       "      <td>non-us-no-city</td>\n",
       "      <td>non-us-no-state</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>lc_002</td>\n",
       "      <td>236</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>non-us-no-state-code</td>\n",
       "      <td>non-us-no-city</td>\n",
       "      <td>non-us-no-state</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>lc_003</td>\n",
       "      <td>101</td>\n",
       "      <td>Albania</td>\n",
       "      <td>non-us-no-state-code</td>\n",
       "      <td>non-us-no-city</td>\n",
       "      <td>non-us-no-state</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>lc_004</td>\n",
       "      <td>316</td>\n",
       "      <td>Algeria</td>\n",
       "      <td>non-us-no-state-code</td>\n",
       "      <td>non-us-no-city</td>\n",
       "      <td>non-us-no-state</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>lc_005</td>\n",
       "      <td>102</td>\n",
       "      <td>Andorra</td>\n",
       "      <td>non-us-no-state-code</td>\n",
       "      <td>non-us-no-city</td>\n",
       "      <td>non-us-no-state</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  location_code_id  country_code      country            state_code  \\\n",
       "0           lc_001           582       Mexico  non-us-no-state-code   \n",
       "1           lc_002           236  Afghanistan  non-us-no-state-code   \n",
       "2           lc_003           101      Albania  non-us-no-state-code   \n",
       "3           lc_004           316      Algeria  non-us-no-state-code   \n",
       "4           lc_005           102      Andorra  non-us-no-state-code   \n",
       "\n",
       "             city            state  \n",
       "0  non-us-no-city  non-us-no-state  \n",
       "1  non-us-no-city  non-us-no-state  \n",
       "2  non-us-no-city  non-us-no-state  \n",
       "3  non-us-no-city  non-us-no-state  \n",
       "4  non-us-no-city  non-us-no-state  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display Location Codes Data in Pandas to View Column Headers & Data Types\n",
    "location_codes_df.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### 2.2.2 Conclusions of Location Codes Data Analysis\n",
    "    A) Main table will need to be split into Country & State Temptables within the EMR Process\n",
    "    B) Country Codes can be isolated using \".filter(location_codes_df.country != 'United States')\"\n",
    "    C) City Codes will need to be mapped to State Codes for aggregate analytics\n",
    "    D) State Codes can be isolated using \".filter(location_codes_df.country == 'United States')\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### 2.2.3 Location Codes Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Write Dataframe to Temptable for use in creating the split tables\n",
    "location_codes_df.createOrReplaceTempView(\"location_codes_temptable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+------------+-----------+--------------------+--------------+---------------+\n",
      "|location_code_id|country_code|    country|          state_code|          city|          state|\n",
      "+----------------+------------+-----------+--------------------+--------------+---------------+\n",
      "|          lc_001|         582|     Mexico|non-us-no-state-code|non-us-no-city|non-us-no-state|\n",
      "|          lc_002|         236|Afghanistan|non-us-no-state-code|non-us-no-city|non-us-no-state|\n",
      "|          lc_003|         101|    Albania|non-us-no-state-code|non-us-no-city|non-us-no-state|\n",
      "|          lc_004|         316|    Algeria|non-us-no-state-code|non-us-no-city|non-us-no-state|\n",
      "|          lc_005|         102|    Andorra|non-us-no-state-code|non-us-no-city|non-us-no-state|\n",
      "+----------------+------------+-----------+--------------------+--------------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Filter & Visualize, then Create Country Codes Temptable\n",
    "country_location_codes_df = location_codes_df.filter(location_codes_df.country != 'United States')\n",
    "country_location_codes_df.show(5)\n",
    "country_location_codes_df.createOrReplaceTempView(\"country_codes_temptable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+------------+-------------+----------+--------------------+-----+\n",
      "|location_code_id|country_code|      country|state_code|                city|state|\n",
      "+----------------+------------+-------------+----------+--------------------+-----+\n",
      "|          lc_290|           1|United States|       ALC|               Alcan|   AK|\n",
      "|          lc_291|           1|United States|       ANC|           Anchorage|   AK|\n",
      "|          lc_292|           1|United States|       BAR|Baker Aaf - Baker...|   AK|\n",
      "|          lc_293|           1|United States|       DAC|       Daltons Cache|   AK|\n",
      "|          lc_294|           1|United States|       PIZ|Dew Station Pt La...|   AK|\n",
      "+----------------+------------+-------------+----------+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Filter & Visualize, then Create State Codes Temptable\n",
    "state_location_codes_df = location_codes_df.filter(location_codes_df.country == 'United States')\n",
    "state_location_codes_df.show(5)\n",
    "state_location_codes_df.createOrReplaceTempView(\"state_codes_temptable\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 2.3.1 Temperature Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Read in Temperature Data\n",
    "temps_file = 'raw_data/all_temps_farenheit.csv'\n",
    "temp_orig_df = spark.read\\\n",
    "    .option(\"header\", \"true\")\\\n",
    "    .option(\"inferSchema\", \"true\")\\\n",
    "    .csv(temps_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- datetime: string (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- avg_temp: double (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- state_name: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Visualize Temperature Data\n",
    "temp_orig_df.head()\n",
    "temp_orig_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datetime</th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "      <th>avg_temp</th>\n",
       "      <th>city</th>\n",
       "      <th>state_name</th>\n",
       "      <th>state</th>\n",
       "      <th>country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1/1/15</td>\n",
       "      <td>1</td>\n",
       "      <td>2015</td>\n",
       "      <td>13.94</td>\n",
       "      <td>Alexander Lake AK US</td>\n",
       "      <td>Alaska</td>\n",
       "      <td>AK</td>\n",
       "      <td>United States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1/1/15</td>\n",
       "      <td>1</td>\n",
       "      <td>2015</td>\n",
       "      <td>-6.17</td>\n",
       "      <td>American Creek AK US</td>\n",
       "      <td>Alaska</td>\n",
       "      <td>AK</td>\n",
       "      <td>United States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1/1/15</td>\n",
       "      <td>1</td>\n",
       "      <td>2015</td>\n",
       "      <td>24.19</td>\n",
       "      <td>Anchor River Divide AK US</td>\n",
       "      <td>Alaska</td>\n",
       "      <td>AK</td>\n",
       "      <td>United States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1/1/15</td>\n",
       "      <td>1</td>\n",
       "      <td>2015</td>\n",
       "      <td>20.97</td>\n",
       "      <td>Anchorage Hillside AK US</td>\n",
       "      <td>Alaska</td>\n",
       "      <td>AK</td>\n",
       "      <td>United States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1/1/15</td>\n",
       "      <td>1</td>\n",
       "      <td>2015</td>\n",
       "      <td>1.19</td>\n",
       "      <td>ANGEL CREEK ALASKA AK US</td>\n",
       "      <td>Alaska</td>\n",
       "      <td>AK</td>\n",
       "      <td>United States</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  datetime  month  year  avg_temp                       city state_name state  \\\n",
       "0   1/1/15      1  2015     13.94       Alexander Lake AK US     Alaska    AK   \n",
       "1   1/1/15      1  2015     -6.17       American Creek AK US     Alaska    AK   \n",
       "2   1/1/15      1  2015     24.19  Anchor River Divide AK US     Alaska    AK   \n",
       "3   1/1/15      1  2015     20.97   Anchorage Hillside AK US     Alaska    AK   \n",
       "4   1/1/15      1  2015      1.19   ANGEL CREEK ALASKA AK US     Alaska    AK   \n",
       "\n",
       "         country  \n",
       "0  United States  \n",
       "1  United States  \n",
       "2  United States  \n",
       "3  United States  \n",
       "4  United States  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display Temperature Data in Pandas to View Column Headers & Data Types\n",
    "temp_orig_df.filter(col(\"country\")==\"United States\").limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### 2.3.2 Conclusions of Location Codes Data Analysis\n",
    "    A) Temperature information can only be aggregated to match the State & Country of i94 Visitor Records\n",
    "    B) Temperature information is useful in aggregate at the State and will need to be mapped to locations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### 2.3.3 Temperatures Data Cleansing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Leverage GroupBy Statements & Aggregate Functions to Identify Usable Data Sets for Analytics\n",
    "temp_grp_avg_df = temp_orig_df.groupBy(\"country\",\"state\",\"month\").agg((avg(\"avg_temp\")), count(\"*\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Format Data Set to Match Intended Analytics Star Schema\n",
    "temp_avg_df = temp_grp_avg_df.withColumnRenamed('avg(avg_temp)','avg_temp').withColumnRenamed('count(1)','stat_count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1173"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Output Count of Data to Validate Transformations\n",
    "temp_avg_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Create Temperatures Temptable for Future Transformations\n",
    "temp_avg_df.createOrReplaceTempView(\"temp_avg_temptable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Join Locations & Temperatures Tables using State Codes \n",
    "temperature_state_dim_df = spark.sql(\n",
    "    'SELECT \\\n",
    "        ta.country, \\\n",
    "        ta.state as temp_state, \\\n",
    "        ta.month, \\\n",
    "        ta.avg_temp, \\\n",
    "        ta.stat_count, \\\n",
    "        sc.city, \\\n",
    "        sc.state, \\\n",
    "        sc.state_code as i94_state_code, \\\n",
    "        sc.location_code_id as state_loc_id \\\n",
    "        FROM temp_avg_temptable ta \\\n",
    "             LEFT JOIN state_codes_temptable sc on ta.state = sc.state')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Create Temperatures_by_State Temptable for Country Codes Append\n",
    "temperature_state_dim_df.createOrReplaceTempView(\"temp_by_state_temptable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Join Locations Temptable & Temperatures_by_State Temptable to Create Final Temperatures Dimension Table\n",
    "temperature_dim = spark.sql(\n",
    "    'SELECT \\\n",
    "        monotonically_increasing_id() as temperature_id, \\\n",
    "        tst.month as temp_month, \\\n",
    "        tst.i94_state_code, \\\n",
    "        cct.country_code as i94_country_code,\\\n",
    "        tst.avg_temp as temp_average, \\\n",
    "        tst.stat_count, \\\n",
    "        tst.city as temp_city, \\\n",
    "        tst.state as temp_state, \\\n",
    "        tst.country as temp_country\\\n",
    "        FROM temp_by_state_temptable tst \\\n",
    "             LEFT JOIN country_codes_temptable cct on tst.country = cct.country')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- temperature_id: long (nullable = false)\n",
      " |-- temp_month: integer (nullable = true)\n",
      " |-- i94_state_code: string (nullable = true)\n",
      " |-- i94_country_code: integer (nullable = true)\n",
      " |-- temp_average: double (nullable = true)\n",
      " |-- stat_count: long (nullable = false)\n",
      " |-- temp_city: string (nullable = true)\n",
      " |-- temp_state: string (nullable = true)\n",
      " |-- temp_country: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Visualize Transformed Data, Validate Appends, and Create Final Temptable for use in Future Appends\n",
    "temperature_dim.count()\n",
    "temperature_dim.printSchema()\n",
    "temperature_dim.createOrReplaceTempView(\"temperatures_dim_table\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 2.4.1 Demographics Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Read in Demographics Data\n",
    "demo_file = 'raw_data/us_cities_demographics.csv'\n",
    "demo_orig_df = spark.read\\\n",
    "    .option(\"header\", \"true\")\\\n",
    "    .option(\"inferSchema\", \"true\")\\\n",
    "    .csv(demo_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- city: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- median_age: double (nullable = true)\n",
      " |-- male_population: integer (nullable = true)\n",
      " |-- female_population: integer (nullable = true)\n",
      " |-- total_population: integer (nullable = true)\n",
      " |-- number_of_veterans: integer (nullable = true)\n",
      " |-- foreign_born: integer (nullable = true)\n",
      " |-- average_household_size: double (nullable = true)\n",
      " |-- state_code: string (nullable = true)\n",
      " |-- race: string (nullable = true)\n",
      " |-- count: integer (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2891"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Visualize Demographics Data\n",
    "demo_orig_df.head()\n",
    "demo_orig_df.printSchema()\n",
    "demo_orig_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+----------+---------------+-----------------+----------------+------------------+------------+----------------------+----------+--------------------+-----+\n",
      "|       city|        state|median_age|male_population|female_population|total_population|number_of_veterans|foreign_born|average_household_size|state_code|                race|count|\n",
      "+-----------+-------------+----------+---------------+-----------------+----------------+------------------+------------+----------------------+----------+--------------------+-----+\n",
      "|     Newark|   New Jersey|      34.6|         138040|           143873|          281913|              5829|       86253|                  2.73|        NJ|               White|76402|\n",
      "|     Peoria|     Illinois|      33.1|          56229|            62432|          118661|              6634|        7517|                   2.4|        IL|American Indian a...| 1343|\n",
      "|   O'Fallon|     Missouri|      36.0|          41762|            43270|           85032|              5783|        3269|                  2.77|        MO|  Hispanic or Latino| 2583|\n",
      "|    Hampton|     Virginia|      35.5|          66214|            70240|          136454|             19638|        6204|                  2.48|        VA|Black or African-...|70303|\n",
      "|   Lakewood|     Colorado|      37.7|          76013|            76576|          152589|              9988|       14169|                  2.29|        CO|  Hispanic or Latino|33630|\n",
      "|       Mesa|      Arizona|      36.9|         234998|           236835|          471833|             31808|       57492|                  2.68|        AZ|American Indian a...|16044|\n",
      "|      Bryan|        Texas|      29.4|          41761|            40345|           82106|              3602|       12014|                  2.55|        TX|Black or African-...|11914|\n",
      "|    Garland|        Texas|      34.5|         116406|           120430|          236836|             10407|       62975|                  3.12|        TX|               Asian|27217|\n",
      "|Springfield|     Illinois|      38.8|          55639|            62170|          117809|              7525|        4264|                  2.22|        IL|               Asian| 3871|\n",
      "|      Flint|     Michigan|      35.3|          48984|            49313|           98297|              3757|        2138|                  2.38|        MI|               Asian|  657|\n",
      "|     Tacoma|   Washington|      37.7|         100914|           107036|          207950|             19040|       31863|                  2.48|        WA|Black or African-...|30914|\n",
      "|  Waterbury|  Connecticut|      36.2|          52235|            56572|          108807|              3493|       19967|                  2.71|        CT|               White|69075|\n",
      "|      Eagan|    Minnesota|      36.8|          31587|            34701|           66288|              2699|        8642|                  2.49|        MN|  Hispanic or Latino| 4328|\n",
      "| Fort Smith|     Arkansas|      34.9|          43346|            44849|           88195|              3408|       13177|                  2.44|        AR|Black or African-...| 9851|\n",
      "| Carmichael|   California|      41.0|          29281|            33934|           63215|              4225|        7378|                  2.39|        CA|               White|55862|\n",
      "|  Daly City|   California|      39.7|          53817|            52757|          106574|              3782|       56640|                  3.26|        CA|               White|25522|\n",
      "|      Bryan|        Texas|      29.4|          41761|            40345|           82106|              3602|       12014|                  2.55|        TX|American Indian a...| 3557|\n",
      "|   Murrieta|   California|      35.3|          49363|            60453|          109816|              7242|       19875|                   3.3|        CA|  Hispanic or Latino|32944|\n",
      "|  Brentwood|     New York|      34.2|          31395|            32397|           63792|              1492|       27058|                  4.98|        NY|American Indian a...| 4242|\n",
      "|     Quincy|Massachusetts|      41.0|          44129|            49500|           93629|              4147|       32935|                  2.39|        MA|American Indian a...|  351|\n",
      "|    Boulder|     Colorado|      29.0|          56342|            51000|          107342|              4061|       12993|                  2.24|        CO|Black or African-...| 1615|\n",
      "|  Waterbury|  Connecticut|      36.2|          52235|            56572|          108807|              3493|       19967|                  2.71|        CT|American Indian a...|  492|\n",
      "|   Evanston|     Illinois|      36.8|          34146|            41377|           75523|              2058|       15003|                  2.29|        IL|               White|54496|\n",
      "|     Dothan|      Alabama|      38.9|          32172|            35364|           67536|              6334|        1699|                  2.59|        AL|               Asian| 1175|\n",
      "|    Antioch|   California|      34.0|          54733|            55809|          110542|              5681|       24942|                  3.31|        CA|American Indian a...| 3462|\n",
      "+-----------+-------------+----------+---------------+-----------------+----------------+------------------+------------+----------------------+----------+--------------------+-----+\n",
      "only showing top 25 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Display Demographics Data in Pandas to View Column Headers & Data Types\n",
    "demo_orig_df.show(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### 2.4.2 Conclusions of Demographics Data Analysis\n",
    "    A) Demographics Tables will need to be split into Demographics by State & Ethnicity by State Dimensions\n",
    "    B) Data will need to be Summed or Averaged by State to deliver meaningful insights\n",
    "    C) Location Codes to match the Summaries by State will need to be appended to produce the final Dimension Table Links to the i94 Fact Table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### 2.4.3 Demographics Data Cleansing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### 2.4.3.1 Produce Ethnicity by State Dimension Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# GroupBy & Aggregate Functions for Ethnicity \n",
    "dg_eth_st_df = demo_orig_df.groupBy(\"state_code\",\"race\").agg((avg(\"median_age\")),(avg(\"average_household_size\")),(Fsum(\"count\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Adjust Columns to Match Usable Analytics Names\n",
    "dg_eth_st_lbl_df = dg_eth_st_df \\\n",
    "    .withColumnRenamed('state_code','state')\\\n",
    "    .withColumnRenamed('avg(median_age)','median_age')\\\n",
    "    .withColumnRenamed('avg(average_household_size)','avg_hh_size')\\\n",
    "    .withColumnRenamed('sum(count)','count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+------------------+------------------+------+\n",
      "|state|                race|        median_age|       avg_hh_size| count|\n",
      "+-----+--------------------+------------------+------------------+------+\n",
      "|   NJ|American Indian a...| 35.94444444444444| 2.934444444444445| 11350|\n",
      "|   KY|American Indian a...|             35.95|             2.395|  7772|\n",
      "|   OK|Black or African-...|              33.4|2.5983333333333336|228888|\n",
      "|   PR|               Asian|             39.75|              null|  2687|\n",
      "|   CA|American Indian a...|36.173846153846156|3.0776153846153846|401386|\n",
      "+-----+--------------------+------------------+------------------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Visualize & Validate, then Create Temptable for developing final Ethnicity Dimension Table\n",
    "dg_eth_st_lbl_df.show(5)\n",
    "dg_eth_st_lbl_df.count()\n",
    "dg_eth_st_lbl_df.createOrReplaceTempView(\"dg_eth_st_lbl_temptable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Create Final Ethnicity_by_State Dimension Table\n",
    "eth_st_w_i94_port_code_df = spark.sql(\n",
    "    'SELECT \\\n",
    "        demo.state, \\\n",
    "        demo.race, \\\n",
    "        demo.median_age, \\\n",
    "        demo.avg_hh_size, \\\n",
    "        demo.count, \\\n",
    "        sc.state_code as i94_state_code \\\n",
    "        FROM dg_eth_st_lbl_temptable demo \\\n",
    "             LEFT JOIN state_codes_temptable sc on demo.state = sc.state')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### 2.4.3.2 Product Demographics by State Dimension Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# GroupBy & Aggregate Functions for Demographics by State\n",
    "dg_st_df = demo_orig_df.groupBy(\"state_code\").agg((Fsum(\"median_age\")),(Fsum(\"male_population\")),(Fsum(\"female_population\")),(Fsum(\"total_population\")),(Fsum(\"number_of_veterans\")),(Fsum(\"foreign_born\")),(avg(\"average_household_size\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Adjust Columns to Match Usable Analytics Names\n",
    "dg_st_lbl_df = dg_st_df \\\n",
    "    .withColumnRenamed('state_code','state')\\\n",
    "    .withColumnRenamed('sum(median_age)','median_age')\\\n",
    "    .withColumnRenamed('sum(male_population)','male_population')\\\n",
    "    .withColumnRenamed('sum(female_population)','female_population')\\\n",
    "    .withColumnRenamed('sum(total_population)','total_population')\\\n",
    "    .withColumnRenamed('sum(number_of_veterans)','number_of_veterans')\\\n",
    "    .withColumnRenamed('sum(foreign_born)','foreign_born')\\\n",
    "    .withColumnRenamed('avg(average_household_size)','avg_hh_size')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------------+---------------+-----------------+----------------+------------------+------------+------------------+\n",
      "|state|        median_age|male_population|female_population|total_population|number_of_veterans|foreign_born|       avg_hh_size|\n",
      "+-----+------------------+---------------+-----------------+----------------+------------------+------------+------------------+\n",
      "|   AZ|2803.0000000000014|       11137275|         11360435|        22497710|           1322525|     3411565|          2.774375|\n",
      "|   SC| 811.8000000000002|        1265291|          1321685|         2586976|            163334|      134019| 2.469583333333333|\n",
      "|   LA|1385.0000000000005|        3134990|          3367985|         6502975|            348855|      417095|2.4650000000000003|\n",
      "|   MN|1921.3000000000006|        3478803|          3565362|         7044165|            321738|     1069888| 2.496851851851851|\n",
      "|   NJ|2009.5000000000002|        3423033|          3507991|         6931024|            146632|     2327750| 2.960877192982456|\n",
      "+-----+------------------+---------------+-----------------+----------------+------------------+------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Visualize & Validate, then Create Temptable for developing final Demographics by State Dimension Table\n",
    "dg_st_lbl_df.show(5)\n",
    "dg_st_lbl_df.count()\n",
    "dg_st_lbl_df.createOrReplaceTempView(\"dg_st_lbl_temptable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Create Final Ethnicity_by_State Dimension Table\n",
    "dg_st_w_i94_port_code_df = spark.sql(\n",
    "    'SELECT \\\n",
    "        stdg.state, \\\n",
    "        stdg.median_age, \\\n",
    "        stdg.male_population, \\\n",
    "        stdg.female_population, \\\n",
    "        stdg.total_population, \\\n",
    "        stdg.number_of_veterans, \\\n",
    "        stdg.foreign_born, \\\n",
    "        stdg.avg_hh_size, \\\n",
    "        sc.state_code as i94_state_code \\\n",
    "        FROM dg_st_lbl_temptable stdg \\\n",
    "             LEFT JOIN state_codes_temptable sc on stdg.state = sc.state')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 2.5.1 Airports Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Read in the Airports Data\n",
    "airports_file = 'raw_data/airport-codes.csv'\n",
    "airports_orig_df = spark.read\\\n",
    "    .option(\"header\", \"true\")\\\n",
    "    .option(\"inferSchema\", \"true\")\\\n",
    "    .csv(airports_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------+--------------------+------------+---------+-----------+----------+------------+--------+---------+----------+--------------------+\n",
      "|ident|         type|                name|elevation_ft|continent|iso_country|iso_region|municipality|gps_code|iata_code|local_code|         coordinates|\n",
      "+-----+-------------+--------------------+------------+---------+-----------+----------+------------+--------+---------+----------+--------------------+\n",
      "|  00A|     heliport|   Total Rf Heliport|          11|       NA|         US|     US-PA|    Bensalem|     00A|     null|       00A|-74.9336013793945...|\n",
      "| 00AA|small_airport|Aero B Ranch Airport|        3435|       NA|         US|     US-KS|       Leoti|    00AA|     null|      00AA|-101.473911, 38.7...|\n",
      "| 00AK|small_airport|        Lowell Field|         450|       NA|         US|     US-AK|Anchor Point|    00AK|     null|      00AK|-151.695999146, 5...|\n",
      "| 00AL|small_airport|        Epps Airpark|         820|       NA|         US|     US-AL|     Harvest|    00AL|     null|      00AL|-86.7703018188476...|\n",
      "| 00AR|       closed|Newport Hospital ...|         237|       NA|         US|     US-AR|     Newport|    null|     null|      null| -91.254898, 35.6087|\n",
      "+-----+-------------+--------------------+------------+---------+-----------+----------+------------+--------+---------+----------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Visualize the Airports Data\n",
    "airports_orig_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### 2.5.2 Conclusions of Airports Data Analysis\n",
    "    A) Airports Data will only be relevant to the U.S. City and will need a City & State Code Appended to match the i94 Visits Analytics Table\n",
    "    B) Airports Data will need to be filtered to focus on large, medium, and small airports only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### 2.5.3 Airports Data Cleansing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------+--------------------+------------+---------+-----------+----------+------------+--------+---------+----------+--------------------+----------+\n",
      "|ident|         type|                name|elevation_ft|continent|iso_country|iso_region|municipality|gps_code|iata_code|local_code|         coordinates|state_code|\n",
      "+-----+-------------+--------------------+------------+---------+-----------+----------+------------+--------+---------+----------+--------------------+----------+\n",
      "|  00A|     heliport|   Total Rf Heliport|          11|       NA|         US|     US-PA|    Bensalem|     00A|     null|       00A|-74.9336013793945...|        PA|\n",
      "| 00AA|small_airport|Aero B Ranch Airport|        3435|       NA|         US|     US-KS|       Leoti|    00AA|     null|      00AA|-101.473911, 38.7...|        KS|\n",
      "| 00AK|small_airport|        Lowell Field|         450|       NA|         US|     US-AK|Anchor Point|    00AK|     null|      00AK|-151.695999146, 5...|        AK|\n",
      "| 00AL|small_airport|        Epps Airpark|         820|       NA|         US|     US-AL|     Harvest|    00AL|     null|      00AL|-86.7703018188476...|        AL|\n",
      "| 00AR|       closed|Newport Hospital ...|         237|       NA|         US|     US-AR|     Newport|    null|     null|      null| -91.254898, 35.6087|        AR|\n",
      "+-----+-------------+--------------------+------------+---------+-----------+----------+------------+--------+---------+----------+--------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Isolate the US Airports by City & State, then Visualize the Dataframe\n",
    "airports_us_only_df = airports_orig_df.filter(airports_orig_df.iso_country == 'US')\n",
    "airport_clean_country_df = airports_us_only_df.withColumn(\"state_code\", substring(\"iso_region\", 4, 2))\n",
    "airport_clean_country_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Create Airports Temptable using Isolated Data by US State\n",
    "airport_clean_country_df.createOrReplaceTempView(\"airports_us_state\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Link together Location Codes & Airport Data\n",
    "airports_dim = spark.sql(\n",
    "    'SELECT \\\n",
    "        abs.ident, \\\n",
    "        abs.type,\\\n",
    "        abs.municipality as city,\\\n",
    "        abs.state_code, \\\n",
    "        sc.state_code as i94_port_code \\\n",
    "        FROM airports_us_state abs \\\n",
    "             LEFT JOIN state_codes_temptable sc on (abs.state_code = sc.state) AND (abs.municipality = sc.city)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Filter Dataframe to only include Small, Medium and Large Airports\n",
    "us_airports_size_dim = airports_dim.filter(airports_dim.i94_port_code.isNotNull()).filter(airports_dim.type != \"closed\").filter(airports_dim.type != \"heliport\").filter(airports_dim.type != \"seaplane_base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------+----------+----------+-------------+\n",
      "|ident|         type|      city|state_code|i94_port_code|\n",
      "+-----+-------------+----------+----------+-------------+\n",
      "| 01FA|small_airport|   Orlando|        FL|          ORL|\n",
      "|  04I|small_airport|  Columbus|        OH|          CLM|\n",
      "| 04WA|small_airport|   Spokane|        WA|          SPO|\n",
      "| 07KY|small_airport|Louisville|        KY|          LOU|\n",
      "| 07MT|small_airport|   Glasgow|        MT|          GGW|\n",
      "+-----+-------------+----------+----------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "689"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Validate Final Dataframe\n",
    "us_airports_size_dim.show(5)\n",
    "us_airports_size_dim.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### (Continuted) 2.1.4 Final Processing of i94 Visit Details Fact Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Append Residence Temperature ID to i94 Visit Details\n",
    "i94_w_res_temp_id_df = spark.sql(\n",
    "    'SELECT \\\n",
    "        pft.i94rec, \\\n",
    "        pft.i94_year, \\\n",
    "        pft.i94_month, \\\n",
    "        pft.i94_citizenship, \\\n",
    "        pft.i94_residence, \\\n",
    "        pft.i94_port_of_entry, \\\n",
    "        pft.arrival_date, \\\n",
    "        pft.arrival_mode, \\\n",
    "        pft.arrival_state, \\\n",
    "        pft.departure_date, \\\n",
    "        pft.i94_age, \\\n",
    "        pft.travel_purpose, \\\n",
    "        pft.count, \\\n",
    "        pft.birth_year, \\\n",
    "        pft.gender, \\\n",
    "        pft.visa_type, \\\n",
    "        tdt.temperature_id as i94_residence_temp_id \\\n",
    "        FROM i94_pre_fact_temptable as pft \\\n",
    "            LEFT JOIN temperatures_dim_table as tdt \\\n",
    "                ON (pft.i94_residence = tdt.i94_country_code) \\\n",
    "                    AND (pft.i94_month = tdt.temp_month)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+---------+---------------+-------------+-----------------+------------+------------+-------------+--------------+-------+--------------+-----+----------+------+---------+---------------------+\n",
      "| i94rec|i94_year|i94_month|i94_citizenship|i94_residence|i94_port_of_entry|arrival_date|arrival_mode|arrival_state|departure_date|i94_age|travel_purpose|count|birth_year|gender|visa_type|i94_residence_temp_id|\n",
      "+-------+--------+---------+---------------+-------------+-----------------+------------+------------+-------------+--------------+-------+--------------+-----+----------+------+---------+---------------------+\n",
      "|66199.0|  2016.0|      4.0|          244.0|        244.0|              NYC|     20545.0|         1.0|           ME|       20554.0|   27.0|           1.0|  1.0|    1989.0|  null|       B1|                 null|\n",
      "|66200.0|  2016.0|      4.0|          244.0|        244.0|              NYC|     20545.0|         1.0|           NJ|       20638.0|   22.0|           1.0|  1.0|    1994.0|  null|       B1|                 null|\n",
      "|66201.0|  2016.0|      4.0|          244.0|        244.0|              NYC|     20545.0|         1.0|           NY|       20547.0|   46.0|           2.0|  1.0|    1970.0|  null|       B2|                 null|\n",
      "|66202.0|  2016.0|      4.0|          244.0|        244.0|              NYC|     20545.0|         1.0|           NY|       20547.0|   43.0|           2.0|  1.0|    1973.0|  null|       B2|                 null|\n",
      "|66203.0|  2016.0|      4.0|          244.0|        244.0|              NYC|     20545.0|         1.0|           NY|       20547.0|    7.0|           2.0|  1.0|    2009.0|  null|       B2|                 null|\n",
      "+-------+--------+---------+---------------+-------------+-----------------+------------+------------+-------------+--------------+-------+--------------+-----+----------+------+---------+---------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- i94rec: double (nullable = true)\n",
      " |-- i94_year: double (nullable = true)\n",
      " |-- i94_month: double (nullable = true)\n",
      " |-- i94_citizenship: double (nullable = true)\n",
      " |-- i94_residence: double (nullable = true)\n",
      " |-- i94_port_of_entry: string (nullable = true)\n",
      " |-- arrival_date: double (nullable = true)\n",
      " |-- arrival_mode: double (nullable = true)\n",
      " |-- arrival_state: string (nullable = true)\n",
      " |-- departure_date: double (nullable = true)\n",
      " |-- i94_age: double (nullable = true)\n",
      " |-- travel_purpose: double (nullable = true)\n",
      " |-- count: double (nullable = true)\n",
      " |-- birth_year: double (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- visa_type: string (nullable = true)\n",
      " |-- i94_residence_temp_id: long (nullable = true)\n",
      "\n",
      "+------+--------+---------+---------------+-------------+-----------------+------------+------------+-------------+--------------+-------+--------------+-----+----------+------+---------+---------------------+\n",
      "|i94rec|i94_year|i94_month|i94_citizenship|i94_residence|i94_port_of_entry|arrival_date|arrival_mode|arrival_state|departure_date|i94_age|travel_purpose|count|birth_year|gender|visa_type|i94_residence_temp_id|\n",
      "+------+--------+---------+---------------+-------------+-----------------+------------+------------+-------------+--------------+-------+--------------+-----+----------+------+---------+---------------------+\n",
      "|  52.0|  2016.0|      4.0|          101.0|        112.0|              NYC|     20545.0|         1.0|           NY|       20558.0|   54.0|           2.0|  1.0|    1962.0|     M|       B2|        1529008357379|\n",
      "|  53.0|  2016.0|      4.0|          101.0|        112.0|              LOS|     20545.0|         1.0|         null|       20546.0|   29.0|           2.0|  1.0|    1987.0|     F|       B2|        1529008357379|\n",
      "| 681.0|  2016.0|      4.0|          103.0|        112.0|              CLT|     20545.0|         1.0|           NC|       20559.0|   61.0|           2.0|  1.0|    1955.0|     F|       WT|        1529008357379|\n",
      "| 682.0|  2016.0|      4.0|          103.0|        112.0|              NEW|     20545.0|         1.0|           FL|       20559.0|   52.0|           2.0|  1.0|    1964.0|     M|       WT|        1529008357379|\n",
      "| 683.0|  2016.0|      4.0|          103.0|        112.0|              NEW|     20545.0|         1.0|           NY|       20554.0|   25.0|           2.0|  1.0|    1991.0|     M|       WT|        1529008357379|\n",
      "+------+--------+---------+---------------+-------------+-----------------+------------+------------+-------------+--------------+-------+--------------+-----+----------+------+---------+---------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Validate Append of Residence Temperature ID, then create i94_with_Residence_Temperature_Id Temptable\n",
    "i94_w_res_temp_id_df.show(5)\n",
    "i94_w_res_temp_id_df.count()\n",
    "i94_w_res_temp_id_df.printSchema()\n",
    "i94_w_res_temp_id_df.filter(i94_w_res_temp_id_df.i94_residence_temp_id.isNotNull()).show(5)\n",
    "i94_w_res_temp_id_df.createOrReplaceTempView(\"i94_w_res_temp_id_temptable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Append Port Temperature ID to i94 Visit Details\n",
    "i94_w_res_temp_id_df = spark.sql(\n",
    "    'SELECT \\\n",
    "        i94wt.i94rec, \\\n",
    "        i94wt.i94_year, \\\n",
    "        i94wt.i94_month, \\\n",
    "        i94wt.i94_citizenship, \\\n",
    "        i94wt.i94_residence, \\\n",
    "        i94wt.i94_port_of_entry, \\\n",
    "        i94wt.arrival_date, \\\n",
    "        i94wt.arrival_mode, \\\n",
    "        i94wt.arrival_state, \\\n",
    "        i94wt.departure_date, \\\n",
    "        i94wt.i94_age, \\\n",
    "        i94wt.travel_purpose, \\\n",
    "        i94wt.count, \\\n",
    "        i94wt.birth_year, \\\n",
    "        i94wt.gender, \\\n",
    "        i94wt.visa_type, \\\n",
    "        i94wt.i94_residence_temp_id, \\\n",
    "        tdt.temperature_id as i94_port_temp_id\\\n",
    "        FROM i94_w_res_temp_id_temptable as i94wt \\\n",
    "            LEFT JOIN temperatures_dim_table as tdt \\\n",
    "                ON (i94wt.i94_port_of_entry = tdt.i94_state_code) \\\n",
    "                    AND (i94wt.i94_month = tdt.temp_month)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+---------+---------------+-------------+-----------------+------------+------------+-------------+--------------+-------+--------------+-----+----------+------+---------+---------------------+----------------+\n",
      "|  i94rec|i94_year|i94_month|i94_citizenship|i94_residence|i94_port_of_entry|arrival_date|arrival_mode|arrival_state|departure_date|i94_age|travel_purpose|count|birth_year|gender|visa_type|i94_residence_temp_id|i94_port_temp_id|\n",
      "+--------+--------+---------+---------------+-------------+-----------------+------------+------------+-------------+--------------+-------+--------------+-----+----------+------+---------+---------------------+----------------+\n",
      "| 66206.0|  2016.0|      4.0|          244.0|        244.0|              DAL|     20545.0|         1.0|           TX|       20565.0|   44.0|           2.0|  1.0|    1972.0|     M|       B2|                 null|    292057776193|\n",
      "|286330.0|  2016.0|      4.0|          213.0|        244.0|              DAL|     20546.0|         1.0|           GA|       20561.0|   58.0|           2.0|  1.0|    1958.0|     F|       B2|                 null|    292057776193|\n",
      "|286751.0|  2016.0|      4.0|          244.0|        244.0|              DAL|     20546.0|         1.0|           CO|       20554.0|   48.0|           1.0|  1.0|    1968.0|     M|       B1|                 null|    292057776193|\n",
      "|286752.0|  2016.0|      4.0|          244.0|        244.0|              DAL|     20546.0|         1.0|           GA|       20623.0|   61.0|           2.0|  1.0|    1955.0|     M|       B2|                 null|    292057776193|\n",
      "|286753.0|  2016.0|      4.0|          244.0|        244.0|              DAL|     20546.0|         1.0|           TX|       20594.0|   63.0|           2.0|  1.0|    1953.0|     F|       B2|                 null|    292057776193|\n",
      "+--------+--------+---------+---------------+-------------+-----------------+------------+------------+-------------+--------------+-------+--------------+-----+----------+------+---------+---------------------+----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3096313"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Validate Append of Port Temperature ID\n",
    "i94_w_res_temp_id_df.show(5)\n",
    "i94_w_res_temp_id_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Step 3: Define the Data Model\n",
    "\n",
    "Having worked through analysis and cleaning of the data, it was possible to develop the final ETL Process that would be used to consistently pull data in from sources, then stage information for redshift analysis. As mentioned before, this was an itterative process, with the final Star Schema shown above being the outcome of the analysis and data cleansing. To support the definition of the data model and processing, the following data flow diagram was developed:\n",
    "\n",
    "![ETL Pipeline Dataflow for Pyspark Build through EMR](http://na-sjdemo1.marketo.com/rs/786-GZR-035/images/ETL_Pipeline_Dataflow_for_Pyspark_Build.png \"ETL Pipeline Dataflow for Pyspark Build through EMR\")\n",
    "\n",
    "> External Link to Visual Flow for Building out the data model:      \n",
    "> http://na-sjdemo1.marketo.com/rs/786-GZR-035/images/ETL_Pipeline_Dataflow_for_Pyspark_Build.png\n",
    "\n",
    "#### 3.1 Explanation of the Conceptual Data Model\n",
    "With multiple data sources, a need to standardize primary keys, foreign keys, and make it simple to link together information for redshift analytics the flow stages data sources while preparing data processing to develop the listed dimension tables and central fact table. A few of the key steps included:\n",
    "\n",
    "    Appending the Location ID to the Temperature Table.\n",
    "    Appending the Location ID to the Demographics and Ethnicity Tables.\n",
    "    Appending the Location ID to the Airport.\n",
    "    Appending the Temperature Record ID using the Location ID and Month of Visit.\n",
    "\n",
    "**Key observations about the Current Data Set (Summary of Gaps)**\n",
    "\n",
    "From a statistical and analytical perspective, making conclusions from average or related data rather than the exactly correlated data requires careful consideration. This particular data model had numerous gaps that needed to be considered when building out the current model.\n",
    "\n",
    "1. Information on Location Temperatures needed to be aggregated to the State Level because matching City-specific details did not present a large enough match rate.\n",
    "2. Information on Countries could not be matched back to Airport Details with enough consistency, so international analysis of Airport Sizing could not be developed.\n",
    "3. Overall, the Data Sets between Temperature, Airport Size, Demographics and i94 Travel Details are all from different time periods.\n",
    "\n",
    "With the current data framework in place data engineers can work to locate increasingly aligned datasets in the future to improve accuracy of the conclusions made by analysts.\n",
    "\n",
    "#### 3.2 Mapping Out Data Pipelines\n",
    "As described within the document, the following approach was used to develop the data flow:\n",
    "\n",
    "1. Gather Data Sources\n",
    "2. Analyze Potential Gaps\n",
    "3. Build a First-pass Conceptual Framework\n",
    "4. Test Data Sets & Outline Initial Star Schema\n",
    "5. Develop Initial Data Handling Flow targeting an EMR ETL.\n",
    "6. Iterrate through Pyspark Configurations\n",
    "7. Adjust Star Schema and Data Flow to Match Findings\n",
    "8. Test the EMR ETL within the Local Directory\n",
    "9. Execute the Final EMR ETL to deliver Dimension & Fact Tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Step 4: Run Pipelines to Model the Data \n",
    "\n",
    "### Overview of Data Model\n",
    "\n",
    "#### 4.1.1 Create the data model - EMR ETL\n",
    "Using the Itterative Process above, the EMR ETL was locally developed within the Udacity Pyspark Workspace, then the following steps were used to execute the full EMR ETL.\n",
    "\n",
    "1. Export the \"capstone_etl.py\" & \"dl.cfg\" file to a desktop directory.\n",
    "2. Use AWS IAC to initialize an EMR Cluster.\n",
    "3. Authenticate to the EMR Cluster through Terminal via SSH to import the \"capstone_etl.py\" and \"dl.cfg\" files.\n",
    "4. Launch the \"final_elt.py\" process within the EMR Cluster Terminal.\n",
    "5. Validate the Dimension & Fact Table Data has been processed to the targeted S3 Bucket and Directories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.1.2 - Details of \"capstone_etl.py\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Import Libraries\n",
    "\n",
    "    import configparser\n",
    "    from datetime import datetime\n",
    "    import os\n",
    "    from pyspark.sql import SparkSession\n",
    "    from pyspark.sql.functions import *\n",
    "    from pyspark.sql.functions import sum as Fsum\n",
    "    from pyspark import SparkContext as sc\n",
    "    from pyspark.sql.window import Window\n",
    "    from pyspark.sql.types import IntegerType"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Read config file variables (VALIDATED & COMPLETE)\n",
    "\n",
    "    config = configparser.ConfigParser()\n",
    "    config.read('dl.cfg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Authenticate using environment variables (VALIDATED & COMPLETE)\n",
    "\n",
    "    os.environ['AWS_ACCESS_KEY_ID']=config['AWS']['KEY']\n",
    "    os.environ['AWS_SECRET_ACCESS_KEY']=config['AWS']['SECRET']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Establish main bucket file (VALIDATED & COMPLETE)\n",
    "\n",
    "    global BUCKET\n",
    "    BUCKET = config.get(\"S3\", \"OUTPUT_BUCKET\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Create spark session (VALIDATED & COMPLETE)\n",
    "\n",
    "    def create_spark_session():\n",
    "\n",
    "        \"\"\"This function XYZ is used to instantiate the spark session and creates a session application named 'i94_Immigration_Schema'.\"\"\"\n",
    "\n",
    "        spark = SparkSession \\\n",
    "            .builder \\\n",
    "            .config(\"spark.jars.packages\",\"saurfang:spark-sas7bdat:2.0.0-s_2.11\") \\\n",
    "            .appName(\"i94_Immigration_Schema\") \\\n",
    "            .enableHiveSupport() \\\n",
    "            .getOrCreate()\n",
    "\n",
    "        return spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### \"Stage i94 Data\" Function\n",
    "\n",
    "    def stage_i94_data (spark, input_data):\n",
    "\n",
    "        \"\"\"This function:\n",
    "        a) Ingests i94_data from S3 Bucket\n",
    "        b) Reformats i94_data to match target analytics dimensions\n",
    "        c) Publishes i94_data to staging table for future transformations\n",
    "        \"\"\"\n",
    "\n",
    "        # get filepath to i94 data file\n",
    "\n",
    "        i94_data_path = 'i94_data/*.json'\n",
    "\n",
    "        # read i94 data file\n",
    "            # note --> is option \"header\" option inferSchema necessary, given the files being in JSON within S3 buckets???\n",
    "\n",
    "        i94_df = spark.read \\\n",
    "            .option(\"header\", \"true\") \\\n",
    "            .option(\"inferSchema\", \"true\") \\\n",
    "            .json('{}{}'.format(input_data,i94_data_path))    \n",
    "\n",
    "        # Select needed i94 columns from master data set\n",
    "\n",
    "        i94_fact_select = i94_df.select('cicid','i94yr','i94mon','i94cit','i94res','i94port','arrdate','i94mode','i94addr','depdate','i94bir','i94visa','count','biryear','gender','visatype')\n",
    "\n",
    "        # Rename column headers to match final Fact Table Name Schema\n",
    "\n",
    "        i94_pre_fact_table = i94_fact_select\\\n",
    "            .withColumnRenamed('cicid','i94rec')\\\n",
    "            .withColumnRenamed('i94yr','i94_year')\\\n",
    "            .withColumnRenamed('i94mon','i94_month')\\\n",
    "            .withColumnRenamed('i94cit','i94_citizenship')\\\n",
    "            .withColumnRenamed('i94res','i94_residence')\\\n",
    "            .withColumnRenamed('i94port','i94_port_of_entry')\\\n",
    "            .withColumnRenamed('arrdate','arrival_date')\\\n",
    "            .withColumnRenamed('i94mode','arrival_mode')\\\n",
    "            .withColumnRenamed('i94addr','arrival_state')\\\n",
    "            .withColumnRenamed('depdate','departure_date')\\\n",
    "            .withColumnRenamed('i94bir','i94_age')\\\n",
    "            .withColumnRenamed('i94visa','travel_purpose')\\\n",
    "            .withColumnRenamed('count','count')\\\n",
    "            .withColumnRenamed('biryear','birth_year')\\\n",
    "            .withColumnRenamed('gender','gender')\\\n",
    "            .withColumnRenamed('visatype','visa_type')\n",
    "\n",
    "        # Store Temptable for Future Processing of Final i94 Fact Table\n",
    "\n",
    "        i94_pre_fact_table.createOrReplaceTempView(\"i94_pre_fact_temptable\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### \"Process Location Codes\" Function\n",
    "\n",
    "    def process_location_codes (spark, input_data):\n",
    "\n",
    "        \"\"\"This function:\n",
    "        a) Ingests Location Data\n",
    "        b) Creates temptable for pre-processing of location data \n",
    "        c) Splits Location Data into Country (not in US) & State (within US) processing files\n",
    "        d) Writes State & Country Location Files to Temptables\n",
    "        \"\"\"\n",
    "\n",
    "        # Get filepath to location data file\n",
    "\n",
    "        location_file = 'location_codes.csv'\n",
    "\n",
    "        # Read the location_data from S3 Bucket\n",
    "\n",
    "        location_codes_df = spark.read\\\n",
    "            .option(\"header\", \"true\")\\\n",
    "            .option(\"inferSchema\", \"true\")\\\n",
    "            .csv('{}{}'.format(input_data,location_file))\n",
    "\n",
    "        # Write original location data to temptable\n",
    "\n",
    "        location_codes_df.createOrReplaceTempView(\"location_codes_temptable\")\n",
    "\n",
    "\n",
    "        # Establish Country Location_Codes Dataframe\n",
    "\n",
    "        country_location_codes_df = location_codes_df.filter(location_codes_df.country != 'United States')\n",
    "\n",
    "\n",
    "        # Create Country_Codes_Temptable\n",
    "\n",
    "        country_location_codes_df.createOrReplaceTempView(\"country_codes_temptable\")\n",
    "\n",
    "\n",
    "        # Establish State Location_Codes Dataframe\n",
    "\n",
    "        state_location_codes_df = location_codes_df.filter(location_codes_df.country == 'United States')\n",
    "\n",
    "\n",
    "        # Create State_Codes Temptable\n",
    "\n",
    "        state_location_codes_df.createOrReplaceTempView(\"state_codes_temptable\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### \"Process Temperature Data\" Function\n",
    "\n",
    "    def process_temperature_data (spark, input_data, output_data):\n",
    "\n",
    "    \"\"\"This function:\n",
    "    a) Ingests Temperature Data\n",
    "    b) Calculates Average Temperatures, Grouped by Country, State & Month\n",
    "    c) Formats Data for Analytics Consumption\n",
    "    d) Write Average Temperatures to Temptable\n",
    "    e) Appends State Codes to Average Temperature Table\n",
    "    f) Creates Temperature by State Temptable\n",
    "    g) Appends Country Codes to Temperature Table\n",
    "    h) Creates Finel temperatures_dim Temptable\n",
    "    i) Writes temperatures_dim to targeted S3 Bucket\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    # Get filepath to temperature data\n",
    "    \n",
    "    temps_file = 'all_temps_farenheit.csv'\n",
    "    \n",
    "    # Read the temperature data from S3 Bucket\n",
    "    \n",
    "    temp_orig_df = spark.read\\\n",
    "        .option(\"header\", \"true\")\\\n",
    "        .option(\"inferSchema\", \"true\")\\\n",
    "        .csv('{}{}'.format(input_data,temps_file))\n",
    "    \n",
    "    # Calculate Average Temperatures, Grouped by Country + State + Month\n",
    "    \n",
    "    temp_grp_avg_df = temp_orig_df.groupBy(\"country\",\"state\",\"month\").agg((avg(\"avg_temp\")), count(\"*\"))\n",
    "    \n",
    "    # Adjust Column Headers with Averages Dataframe\n",
    "    \n",
    "    temp_avg_df = temp_grp_avg_df.withColumnRenamed('avg(avg_temp)','avg_temp').withColumnRenamed('count(1)','stat_count')\n",
    "    \n",
    "    # Write Average Temperatures to Temptable\n",
    "    \n",
    "    temp_avg_df.createOrReplaceTempView(\"temp_avg_temptable\")\n",
    "    \n",
    "    \n",
    "    # Append State Codes to Average Temperature Temptable\n",
    "    \n",
    "    temperature_state_dim_df = spark.sql(\n",
    "        'SELECT \\\n",
    "            ta.country, \\\n",
    "            ta.state as temp_state, \\\n",
    "            ta.month, \\\n",
    "            ta.avg_temp, \\\n",
    "            ta.stat_count, \\\n",
    "            sc.city, \\\n",
    "            sc.state, \\\n",
    "            sc.state_code as i94_state_code, \\\n",
    "            sc.location_code_id as state_loc_id \\\n",
    "            FROM temp_avg_temptable ta \\\n",
    "                 LEFT JOIN state_codes_temptable sc on ta.state = sc.state')\n",
    "    \n",
    "    # Create Temperature_by_State Temptable    \n",
    "    temperature_state_dim_df.createOrReplaceTempView(\"temp_by_state_temptable\")\n",
    "    \n",
    "    \n",
    "    # Append Country Codes to Average Temperature Temptable    \n",
    "    temperature_dim = spark.sql(\n",
    "        'SELECT \\\n",
    "            monotonically_increasing_id() as temperature_id, \\\n",
    "            tst.month as temp_month, \\\n",
    "            tst.i94_state_code, \\\n",
    "            cct.country_code as i94_country_code,\\\n",
    "            tst.avg_temp as temp_average, \\\n",
    "            tst.stat_count, \\\n",
    "            tst.city as temp_city, \\\n",
    "            tst.state as temp_state, \\\n",
    "            tst.country as temp_country\\\n",
    "            FROM temp_by_state_temptable tst \\\n",
    "                 LEFT JOIN country_codes_temptable cct on tst.country = cct.country')\n",
    "    \n",
    "    # Create Final temperatures_dim Temptable\n",
    "    temperature_dim.createOrReplaceTempView(\"temperatures_dim_table\")\n",
    "    \n",
    "    \n",
    "    # write temperatures_dim table to parquet files partitioned by month\n",
    "    temperature_dim.write.partitionBy(\"temp_month\").mode('overwrite').parquet(\"temperature_dim.parquet\")\n",
    "    \n",
    "    \n",
    "    # write temperatures_dim table to s3 bucket file\n",
    "    temperature_dim.write.json('{}temperature_dim'.format(output_data), mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### \"Process Demographics Data\" Function\n",
    "\n",
    "    def process_demographics_data(spark, input_data, output_data):\n",
    "\n",
    "        \"\"\"This function:\n",
    "        a) Ingests Demographics Data\n",
    "        b) Splits Demographics Data into Ethnicity & Demographics by State Temptables\n",
    "        c) Adjusts Columns to Analytics-friendly Names\n",
    "        d) Writes Ethnicity_dim & Demographics_dim to targeted S3 Buckets\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # Get filepath to Demographics Data\n",
    "\n",
    "        dg_file = 'us_cities_demographics.csv'\n",
    "\n",
    "        # Read Demographics Data from S3 Bucket\n",
    "\n",
    "        dg_orig_df = spark.read\\\n",
    "            .option(\"header\", \"true\")\\\n",
    "            .option(\"inferSchema\", \"true\")\\\n",
    "            .csv('{}{}'.format(input_data,dg_file))\n",
    "\n",
    "\n",
    "        # Split Demographics Data into Ethnicity File\n",
    "\n",
    "        dg_eth_st_df = dg_orig_df.groupBy(\"state_code\",\"race\")\\\n",
    "            .agg((avg(\"median_age\")),\\\n",
    "                 (avg(\"average_household_size\")),\\\n",
    "                 (Fsum(\"count\")))\n",
    "\n",
    "\n",
    "        # Adjust Column Names to match Analytics-friendly Schema Names\n",
    "\n",
    "        dg_eth_st_lbl_df = dg_eth_st_df \\\n",
    "            .withColumnRenamed('state_code','state')\\\n",
    "            .withColumnRenamed('avg(median_age)','median_age')\\\n",
    "            .withColumnRenamed('avg(average_household_size)','avg_hh_size')\\\n",
    "            .withColumnRenamed('sum(count)','count')\n",
    "\n",
    "        # Create Ethnicity_by_State Temptable\n",
    "\n",
    "        dg_eth_st_lbl_df.createOrReplaceTempView(\"dg_eth_st_lbl_temptable\")\n",
    "\n",
    "        # Append State_Code to Ethnicity_by_State Temptable\n",
    "\n",
    "        eth_st_w_i94_port_code_df = spark.sql(\n",
    "            'SELECT \\\n",
    "                demo.state, \\\n",
    "                demo.race, \\\n",
    "                demo.median_age, \\\n",
    "                demo.avg_hh_size, \\\n",
    "                demo.count, \\\n",
    "                sc.state_code as i94_state_code \\\n",
    "                FROM dg_eth_st_lbl_temptable demo \\\n",
    "                     LEFT JOIN state_codes_temptable sc on demo.state = sc.state')\n",
    "\n",
    "\n",
    "        # Output Ethnicity_by_State to Dimension Table\n",
    "\n",
    "        # Create Final eth_st_w_i94_port_code_dim Temptable\n",
    "        eth_st_w_i94_port_code_df.createOrReplaceTempView(\"eth_st_w_i94_port_code_dim_table\")\n",
    "\n",
    "\n",
    "        # write ethnicity table to parquet files partitioned by state\n",
    "        eth_st_w_i94_port_code_df.write.partitionBy(\"state\").mode('overwrite').parquet(\"eth_st_w_i94_port_code_dim.parquet\")\n",
    "\n",
    "\n",
    "        # write ethnicity table to s3 bucket file \"\"\n",
    "        eth_st_w_i94_port_code_df.write.json('{}eth_st_w_i94_port_code_dim'.format(output_data), mode=\"overwrite\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # Create Aggregate Demographics by State\n",
    "\n",
    "        dg_st_df = dg_orig_df.groupBy(\"state_code\") \\\n",
    "            .agg((Fsum(\"median_age\")), \\\n",
    "                 (Fsum(\"male_population\")), \\\n",
    "                 (Fsum(\"female_population\")), \\\n",
    "                 (Fsum(\"total_population\")), \\\n",
    "                 (Fsum(\"number_of_veterans\")), \\\n",
    "                 (Fsum(\"foreign_born\")), \\\n",
    "                 (avg(\"average_household_size\")))\n",
    "\n",
    "        # Adjust Column Names to match Analytics-friendly Schema Names\n",
    "\n",
    "        dg_st_lbl_df = dg_st_df \\\n",
    "            .withColumnRenamed('state_code','state')\\\n",
    "            .withColumnRenamed('sum(median_age)','median_age')\\\n",
    "            .withColumnRenamed('sum(male_population)','male_population')\\\n",
    "            .withColumnRenamed('sum(female_population)','female_population')\\\n",
    "            .withColumnRenamed('sum(total_population)','total_population')\\\n",
    "            .withColumnRenamed('sum(number_of_veterans)','number_of_veterans')\\\n",
    "            .withColumnRenamed('sum(foreign_born)','foreign_born')\\\n",
    "            .withColumnRenamed('avg(average_household_size)','avg_hh_size')\n",
    "\n",
    "        # Create Intermediate Demographics_by_State_Label Temptable\n",
    "\n",
    "        dg_st_lbl_df.createOrReplaceTempView(\"dg_st_lbl_temptable\")\n",
    "\n",
    "        # Append State_Code to Demographics_by_State Temptable\n",
    "\n",
    "        dg_st_w_i94_port_code_df = spark.sql(\n",
    "            'SELECT \\\n",
    "                stdg.state, \\\n",
    "                stdg.median_age, \\\n",
    "                stdg.male_population, \\\n",
    "                stdg.female_population, \\\n",
    "                stdg.total_population, \\\n",
    "                stdg.number_of_veterans, \\\n",
    "                stdg.foreign_born, \\\n",
    "                stdg.avg_hh_size, \\\n",
    "                sc.state_code as i94_state_code \\\n",
    "                FROM dg_st_lbl_temptable stdg \\\n",
    "                     LEFT JOIN state_codes_temptable sc on stdg.state = sc.state')\n",
    "\n",
    "\n",
    "        # Output Aggregate_Demographics_by_State to Dimension Table\n",
    "\n",
    "        # Create Final eth_st_w_i94_port_code_dim Temptable\n",
    "        dg_st_w_i94_port_code_df.createOrReplaceTempView(\"dg_st_w_i94_port_code_dim_table\")\n",
    "\n",
    "\n",
    "        # write demographics table to parquet files partitioned by state\n",
    "        dg_st_w_i94_port_code_df.write.partitionBy(\"state\").mode('overwrite').parquet(\"dg_st_w_i94_port_code_dim.parquet\")\n",
    "\n",
    "\n",
    "        # write demographics table to s3 bucket file \"\"\n",
    "        dg_st_w_i94_port_code_df.write.json('{}dg_st_w_i94_port_code_dim'.format(output_data), mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### \"Process Airports Data\" Function\n",
    "\n",
    "    def process_airports_data (spark, input_data, output_data):\n",
    "\n",
    "        \"\"\"This function:\n",
    "        a) Ingests Airports Data\n",
    "        b) Filters Airports to Only US Locations\n",
    "        c) Filters Airports for Only Large, Medium, and Small Airports Data Set\n",
    "        d) Appends Location Codes\n",
    "        e) Exports Airports Data to targeted S3 Bucket\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # Get filepath to temperature data\n",
    "\n",
    "        airports_file = 'airport-codes.csv'\n",
    "\n",
    "        # Read the temperature data from S3 Bucket\n",
    "\n",
    "        airports_orig_df = spark.read\\\n",
    "            .option(\"header\", \"true\")\\\n",
    "            .option(\"inferSchema\", \"true\")\\\n",
    "            .csv('{}{}'.format(input_data,airports_file))\n",
    "\n",
    "        # Filter for only US Airports Data\n",
    "\n",
    "        airports_us_only_df = airports_orig_df.filter(airports_orig_df.iso_country == 'US')\n",
    "\n",
    "        # Create Airports Data \"state_code\" Column using substring function\n",
    "\n",
    "        airport_clean_country_df = airports_us_only_df.withColumn(\"state_code\", substring(\"iso_region\", 4, 2))\n",
    "\n",
    "        # Create Airports_by_US_State Temptable\n",
    "\n",
    "        airport_clean_country_df.createOrReplaceTempView(\"airports_us_state\")\n",
    "\n",
    "        # Append i94_State_Code Using i94_Location_State_Code Temptable\n",
    "\n",
    "        airports_dim = spark.sql(\n",
    "        'SELECT \\\n",
    "            abs.ident, \\\n",
    "            abs.type,\\\n",
    "            abs.municipality as city,\\\n",
    "            abs.state_code, \\\n",
    "            sc.state_code as i94_port_code \\\n",
    "            FROM airports_us_state abs \\\n",
    "                 LEFT JOIN state_codes_temptable sc on (abs.state_code = sc.state) AND (abs.municipality = sc.city)')\n",
    "\n",
    "        # Filter out Extraneous Airport Type Values\n",
    "\n",
    "        us_airports_size_dim = airports_dim.filter(airports_dim.i94_port_code.isNotNull()) \\\n",
    "            .filter(airports_dim.type != \"closed\") \\\n",
    "            .filter(airports_dim.type != \"heliport\") \\\n",
    "            .filter(airports_dim.type != \"seaplane_base\")\n",
    "\n",
    "\n",
    "        # Output Airports Types to Dimension Table\n",
    "\n",
    "        # Create Final us_airports_size_dim Temptable\n",
    "        us_airports_size_dim.createOrReplaceTempView(\"us_airports_size_dim_table\")\n",
    "\n",
    "\n",
    "        # write airports table to parquet files partitioned by state\n",
    "        us_airports_size_dim.write.partitionBy(\"state_code\").mode('overwrite').parquet(\"us_airports_size_dim.parquet\")\n",
    "\n",
    "\n",
    "        # write aiports table to s3 bucket file \"\"\n",
    "        us_airports_size_dim.write.json('{}us_airports_size_dim'.format(output_data), mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### \"Process i94_fact_table\" Function\n",
    "\n",
    "    def process_i94_fact_table (spark, output_data):\n",
    "\n",
    "        \"\"\"This function:\n",
    "        a) Processes i94 Temptables to Create Final i94 Travel Details Fact Table\n",
    "        b) Appends Country-specific Temperature ID as Residence Temp ID\n",
    "        c) Appends Port-specific Temperature ID as Port Temp ID\n",
    "        d) Write final i94_travel_details Fact Table to targeted S3 Bucket\n",
    "\n",
    "        \"\"\"    \n",
    "\n",
    "        # ???Does this function require that the outside variable of \"tempertures_dim_table\" be a variable???\n",
    "\n",
    "        # Join i94_data with Temperature Codes for Residence\n",
    "\n",
    "        i94_w_res_temp_id_df = spark.sql(\n",
    "            'SELECT \\\n",
    "                pft.i94rec, \\\n",
    "                pft.i94_year, \\\n",
    "                pft.i94_month, \\\n",
    "                pft.i94_citizenship, \\\n",
    "                pft.i94_residence, \\\n",
    "                pft.i94_port_of_entry, \\\n",
    "                pft.arrival_date, \\\n",
    "                pft.arrival_mode, \\\n",
    "                pft.arrival_state, \\\n",
    "                pft.departure_date, \\\n",
    "                pft.i94_age, \\\n",
    "                pft.travel_purpose, \\\n",
    "                pft.count, \\\n",
    "                pft.birth_year, \\\n",
    "                pft.gender, \\\n",
    "                pft.visa_type, \\\n",
    "                tdt.temperature_id as i94_residence_temp_id \\\n",
    "                FROM i94_pre_fact_temptable as pft \\\n",
    "                    LEFT JOIN temperatures_dim_table as tdt \\\n",
    "                        ON (pft.i94_residence = tdt.i94_country_code) \\\n",
    "                            AND (pft.i94_month = tdt.temp_month)')\n",
    "\n",
    "        # Write i94_data_with_residence_temp_codes to Temptable\n",
    "\n",
    "        i94_w_res_temp_id_df.createOrReplaceTempView(\"i94_w_res_temp_id_temptable\")\n",
    "\n",
    "        # Join i94_with_residence_temp_codes with Temperature Codes for i94_Port\n",
    "\n",
    "        i94_w_res_temp_id_df = spark.sql(\n",
    "        'SELECT \\\n",
    "            i94wt.i94rec, \\\n",
    "            i94wt.i94_year, \\\n",
    "            i94wt.i94_month, \\\n",
    "            i94wt.i94_citizenship, \\\n",
    "            i94wt.i94_residence, \\\n",
    "            i94wt.i94_port_of_entry, \\\n",
    "            i94wt.arrival_date, \\\n",
    "            i94wt.arrival_mode, \\\n",
    "            i94wt.arrival_state, \\\n",
    "            i94wt.departure_date, \\\n",
    "            i94wt.i94_age, \\\n",
    "            i94wt.travel_purpose, \\\n",
    "            i94wt.count, \\\n",
    "            i94wt.birth_year, \\\n",
    "            i94wt.gender, \\\n",
    "            i94wt.visa_type, \\\n",
    "            i94wt.i94_residence_temp_id, \\\n",
    "            tdt.temperature_id as i94_port_temp_id\\\n",
    "            FROM i94_w_res_temp_id_temptable as i94wt \\\n",
    "                LEFT JOIN temperatures_dim_table as tdt \\\n",
    "                    ON (i94wt.i94_port_of_entry = tdt.i94_state_code) \\\n",
    "                        AND (i94wt.i94_month = tdt.temp_month)')\n",
    "\n",
    "\n",
    "        # Output Final i94_visits_fact Table\n",
    "\n",
    "        # Create Final us_airports_size_dim Temptable\n",
    "        i94_w_res_temp_id_df.createOrReplaceTempView(\"i94_visits_fact_table\")\n",
    "\n",
    "\n",
    "        # write songs table to parquet files partitioned by state\n",
    "        i94_w_res_temp_id_df.write.partitionBy(\"i94_port_of_entry\").mode('overwrite').parquet(\"i94_visits_fact.parquet\")\n",
    "\n",
    "\n",
    "        # write songs table to s3 bucket file \"\"\n",
    "        i94_w_res_temp_id_df.write.json('{}i94_visits_fact'.format(output_data), mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Launch functions within \"capstone_etl.py\"\n",
    "\n",
    "    def main():\n",
    "\n",
    "        \"\"\"This master function executes all functions within the 'etl.py' file.\"\"\"\n",
    "\n",
    "        spark = create_spark_session()\n",
    "        input_data = 's3://pg-001-1st-test-bucket/capstone/raw_data/'\n",
    "        output_data = BUCKET\n",
    "\n",
    "        stage_i94_data (spark, input_data)\n",
    "        process_location_codes (spark, input_data)\n",
    "        process_temperature_data (spark, input_data, output_data)\n",
    "        process_demographics_data(spark, input_data, output_data)\n",
    "        process_airports_data (spark, input_data, output_data)\n",
    "        process_i94_fact_table (spark, output_data)\n",
    "\n",
    "\n",
    "\n",
    "    if __name__ == \"__main__\":\n",
    "        main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.1.2 Create the Data Pipeline - Apache Airflow\n",
    "\n",
    "To support the ongoing use and update of information within the analytics framework, the finalized data model was translated to a data pipeline through Apache Airflow to upload analytics tables to a redshift cluster and validate that data is loading without errors. A DAG (Directed Acyclic Graph) was developed to support ingestion, processing, and validation of the i94 Travel Details Analytics Star Schema.\n",
    "\n",
    "1. Create tables within the Redshift Instance that mirror the Dimension & Fact Tables within the Star Schema - See \"capstone_create_tables.sql\" file (also included below)\n",
    "2. Log into and Launch Apache Airflow Instance.\n",
    "3. Load AWS Credentials to Airflow.\n",
    "4. Launch Capstone DAG, including Loading and Validation Processes.\n",
    "\n",
    "Accessing & Using the Airflow DAG Files:\n",
    "To Launch the Airflow Files, see the \"Capstone Airflow Files\" folder within this workspace and mirror the folder structure within an Airflow Environment:\n",
    "\n",
    "![Capstone Airflow - Directory Structure](http://na-sjdemo1.marketo.com/rs/786-GZR-035/images/Airflow_Directory_Structure.png)\n",
    "\n",
    "\n",
    "The Airflow Data Pipeline produced the following graph:\n",
    "\n",
    "![Capstone DAG - Graph View](http://na-sjdemo1.marketo.com/rs/786-GZR-035/images/capstone_dag_completed_graph_view.png)\n",
    "\n",
    "> External Link to Airflow Data Pipeline:     \n",
    "> http://na-sjdemo1.marketo.com/rs/786-GZR-035/images/capstone_dag_completed_graph_view.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Capstone Create Tables SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "    CREATE TABLE IF NOT EXISTS i94_visit_details_fact (\n",
    "        \"i94rec\"              int     NOT NULL     PRIMARY KEY,\n",
    "        \"i94_year\"            int,\n",
    "        \"i94_month\"           int,\n",
    "        \"i94_citizenship\"     varchar,\n",
    "        \"i94_residence\"       varchar,\n",
    "        \"i94_port_of_entry\"   varchar sortkey,\n",
    "        \"arrival_date\"        numeric,\n",
    "        \"arrival_mode\"        int,\n",
    "        \"arrival_state\"       varchar,\n",
    "        \"departure_date\"      numeric,\n",
    "        \"i94_age\"             int,\n",
    "        \"travel_purpose\"      int,\n",
    "        \"count\"               int,\n",
    "        \"birth_year\"          int,\n",
    "        \"gender\"              varchar,\n",
    "        \"visa_type\"           varchar,\n",
    "        \"residence_temp_id\"   int,\n",
    "        \"port_temp_id\"        int\n",
    "    );\n",
    "\n",
    "\n",
    "    CREATE TABLE IF NOT EXISTS location_codes_dim (\n",
    "        \"location_code_id\"      varchar     NOT NULL,\n",
    "        \"country_code\"          varchar,\n",
    "        \"country\"               varchar,\n",
    "        \"state_code\"            varchar,\n",
    "        \"city\"                  varchar,\n",
    "        \"state\"                 varchar\n",
    "    );\n",
    "\n",
    "\n",
    "    CREATE TABLE IF NOT EXISTS travel_mode_dim (\n",
    "        \"travel_mode_code\"      varchar     NOT NULL      PRIMARY KEY,\n",
    "        \"mode\"                  varchar\n",
    "    );\n",
    "\n",
    "\n",
    "    CREATE TABLE IF NOT EXISTS travel_purpose_dim (\n",
    "        \"travel_purpose_code\"   varchar     NOT NULL      PRIMARY KEY,\n",
    "        \"travel_purpose\"        varchar\n",
    "    );\n",
    "\n",
    "\n",
    "    CREATE TABLE IF NOT EXISTS visa_type_codes_dim (\n",
    "        \"visa_code\"             varchar     NOT NULL,\n",
    "        \"visa_category\"         varchar,\n",
    "        \"visa_travel_purpose\"   varchar\n",
    "    );\n",
    "\n",
    "\n",
    "    CREATE TABLE IF NOT EXISTS temperatures_dim (\n",
    "        \"temperature_id\"          bigint     NOT NULL   PRIMARY KEY,\n",
    "        \"temp_month\"              int,\n",
    "        \"i94_state_code\"          varchar,\n",
    "        \"i94_country_code\"        varchar,\n",
    "        \"temp_average\"            numeric,\n",
    "        \"stat_count\"              int,\n",
    "        \"temp_city\"               varchar,\n",
    "        \"temp_state\"              varchar,\n",
    "        \"temp_country\"            varchar\n",
    "    );\n",
    "\n",
    "\n",
    "    CREATE TABLE IF NOT EXISTS state_demographics_dim (\n",
    "        \"i94_state_code\"         varchar     NOT NULL     PRIMARY KEY,\n",
    "        \"median_age\"             numeric,\n",
    "        \"male_population\"        int,\n",
    "        \"female_population\"      int,\n",
    "        \"total_population\"       int,\n",
    "        \"number_of_veterans\"     int,\n",
    "        \"foreign_born\"           int,\n",
    "        \"avg_hh_size\"            numeric\n",
    "    );\n",
    "\n",
    "\n",
    "    CREATE TABLE IF NOT EXISTS ethnicity_by_state_dim (\n",
    "        \"i94_state_code\"          varchar,\n",
    "        \"race\"                    varchar,\n",
    "        \"state\"                   varchar,\n",
    "        \"count\"                   int\n",
    "    );\n",
    "\n",
    "\n",
    "    CREATE TABLE IF NOT EXISTS us_airports_size_dim (\n",
    "        \"i94_port_code\"           varchar     NOT NULL     PRIMARY KEY,\n",
    "        \"state_code\"              varchar,\n",
    "        \"type\"                    varchar,\n",
    "        \"city\"                    varchar,\n",
    "        \"ident\"                   varchar\n",
    "    );"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Content of \"capstone_dag.py\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "    from datetime import datetime, timedelta\n",
    "    import os\n",
    "    from airflow import DAG\n",
    "    from airflow.operators.dummy_operator import DummyOperator\n",
    "    from operators import (JsonToRedshiftOperator, CsvToRedshiftOperator, CapstoneDataQualityOperator)\n",
    "    from helpers import SqlQueries\n",
    "\n",
    "    from airflow.contrib.hooks.aws_hook import AwsHook\n",
    "    from airflow.hooks.postgres_hook import PostgresHook\n",
    "    from airflow.operators.postgres_operator import PostgresOperator\n",
    "    from airflow.operators.python_operator import PythonOperator\n",
    "\n",
    "    # Instantiates the AWS Credentials\n",
    "\n",
    "    AWS_KEY = os.environ.get('AWS_KEY')\n",
    "    AWS_SECRET = os.environ.get('AWS_SECRET')\n",
    "\n",
    "\n",
    "    # Creates the Default Args for the DAG\n",
    "\n",
    "    default_args = {\n",
    "        'owner': 'udacity',\n",
    "        'start_date': datetime(2020, 1, 12),\n",
    "        'depends_on_past': False,\n",
    "        'retries': 3,\n",
    "        'retry_delay':timedelta(seconds=300),\n",
    "        'catchup': False,\n",
    "        'email_on_retry': False,\n",
    "        'query_checks': [\n",
    "            {'check_sql': \"SELECT COUNT(*) FROM i94_visit_details_fact WHERE i94rec is null\", 'expected_result':0},\n",
    "            {'check_sql': \"SELECT COUNT(*) FROM location_codes_dim WHERE location_code_id is null\", 'expected_result':0},\n",
    "            {'check_sql': \"SELECT COUNT(*) FROM travel_mode_dim WHERE travel_mode_code is null\", 'expected_result':0},\n",
    "            {'check_sql': \"SELECT COUNT(*) FROM travel_purpose_dim WHERE travel_purpose_code is null\", 'expected_result':0},\n",
    "            {'check_sql': \"SELECT COUNT(*) FROM visa_type_codes_dim WHERE visa_code is null\", 'expected_result':0},\n",
    "            {'check_sql': \"SELECT COUNT(*) FROM temperatures_dim WHERE temperature_id is null\", 'expected_result':0},\n",
    "            {'check_sql': \"SELECT COUNT(*) FROM state_demographics_dim WHERE i94_state_code is null\", 'expected_result':0},\n",
    "            {'check_sql': \"SELECT COUNT(*) FROM ethnicity_by_state_dim WHERE race is null\", 'expected_result':0},\n",
    "            {'check_sql': \"SELECT COUNT(*) FROM us_airports_size_dim WHERE i94_port_code is null\", 'expected_result':0}\n",
    "            ]\n",
    "    }\n",
    "\n",
    "\n",
    "    # Establishes the DAG\n",
    "\n",
    "    dag = DAG(\n",
    "        'capstone_dag',\n",
    "        default_args = default_args,\n",
    "        max_active_runs=1,\n",
    "        description='Load and transform i94_travel_visits in Redshift with Airflow',\n",
    "        schedule_interval='@monthly'\n",
    "    )\n",
    "\n",
    "\n",
    "    # Sets the Start Operator\n",
    "\n",
    "    start_operator = DummyOperator(task_id='Begin_execution',  dag=dag)\n",
    "\n",
    "\n",
    "    # Uses the ImportJsonToRedshift Operator to Load JSON Project Tables\n",
    "\n",
    "    import_i94_visit_details_fact = JsonToRedshiftOperator(\n",
    "        task_id='Import_i94_Visit_Details',\n",
    "        dag=dag,\n",
    "        table=\"i94_visit_details_fact\",\n",
    "        redshift_conn_id=\"redshift\",\n",
    "        aws_credentials_id=\"aws_credentials\",\n",
    "        s3_bucket=\"pg-001-1st-test-bucket\",\n",
    "        data_path=\"capstone/analytics_data/i94_visits_fact\"\n",
    "    )\n",
    "\n",
    "    import_state_demographics_dim = JsonToRedshiftOperator(\n",
    "        task_id='Import_State_Demographics',\n",
    "        dag=dag,\n",
    "        table=\"state_demographics_dim\",\n",
    "        redshift_conn_id=\"redshift\",\n",
    "        aws_credentials_id=\"aws_credentials\",\n",
    "        s3_bucket=\"pg-001-1st-test-bucket\",\n",
    "        data_path=\"capstone/analytics_data/dg_st_w_i94_port_code_dim\"\n",
    "    )\n",
    "\n",
    "    import_ethnicity_by_state = JsonToRedshiftOperator(\n",
    "        task_id='Import_Ethnicity_by_State',\n",
    "        dag=dag,\n",
    "        table=\"ethnicity_by_state_dim\",\n",
    "        redshift_conn_id=\"redshift\",\n",
    "        aws_credentials_id=\"aws_credentials\",\n",
    "        s3_bucket=\"pg-001-1st-test-bucket\",\n",
    "        data_path=\"capstone/analytics_data/eth_st_w_i94_port_code_dim\"\n",
    "    )\n",
    "\n",
    "    import_temperatures_dim = JsonToRedshiftOperator(\n",
    "        task_id='Import_Temperatures',\n",
    "        dag=dag,\n",
    "        table=\"temperatures_dim\",\n",
    "        redshift_conn_id=\"redshift\",\n",
    "        aws_credentials_id=\"aws_credentials\",\n",
    "        s3_bucket=\"pg-001-1st-test-bucket\",\n",
    "        data_path=\"capstone/analytics_data/temperature_dim\"\n",
    "    )\n",
    "\n",
    "    import_us_airports_size_dim = JsonToRedshiftOperator(\n",
    "        task_id='Import_Airports_Sizing',\n",
    "        dag=dag,\n",
    "        table=\"us_airports_size_dim\",\n",
    "        redshift_conn_id=\"redshift\",\n",
    "        aws_credentials_id=\"aws_credentials\",\n",
    "        s3_bucket=\"pg-001-1st-test-bucket\",\n",
    "        data_path=\"capstone/analytics_data/us_airports_size_dim\"\n",
    "    )\n",
    "\n",
    "    # Uses the ImportCSVToRedshift Operator to Load JSON Project Tables\n",
    "\n",
    "    import_location_codes_dim = CsvToRedshiftOperator(\n",
    "        task_id='Import_Location_Codes',\n",
    "        dag=dag,\n",
    "        table=\"location_codes_dim\",\n",
    "        redshift_conn_id=\"redshift\",\n",
    "        aws_credentials_id=\"aws_credentials\",\n",
    "        s3_bucket=\"pg-001-1st-test-bucket\",\n",
    "        data_path=\"capstone/analytics_keys/location_codes.csv\"\n",
    "    )\n",
    "\n",
    "    import_travel_mode_dim = CsvToRedshiftOperator(\n",
    "        task_id='Import_Travel_Mode_Codes',\n",
    "        dag=dag,\n",
    "        table=\"travel_mode_dim\",\n",
    "        redshift_conn_id=\"redshift\",\n",
    "        aws_credentials_id=\"aws_credentials\",\n",
    "        s3_bucket=\"pg-001-1st-test-bucket\",\n",
    "        data_path=\"capstone/analytics_keys/travel_mode_code.csv\"\n",
    "    )\n",
    "\n",
    "    import_travel_purpose_dim = CsvToRedshiftOperator(\n",
    "        task_id='Import_Travel_Purpose',\n",
    "        dag=dag,\n",
    "        table=\"travel_purpose_dim\",\n",
    "        redshift_conn_id=\"redshift\",\n",
    "        aws_credentials_id=\"aws_credentials\",\n",
    "        s3_bucket=\"pg-001-1st-test-bucket\",\n",
    "        data_path=\"capstone/analytics_keys/travel_purpose.csv\"\n",
    "    )\n",
    "\n",
    "    import_visa_type_codes_dim = CsvToRedshiftOperator(\n",
    "        task_id='Import_Visa_Type_Codes',\n",
    "        dag=dag,\n",
    "        table=\"visa_type_codes_dim\",\n",
    "        redshift_conn_id=\"redshift\",\n",
    "        aws_credentials_id=\"aws_credentials\",\n",
    "        s3_bucket=\"pg-001-1st-test-bucket\",\n",
    "        data_path=\"capstone/analytics_keys/visa_type_codes.csv\"\n",
    "    )\n",
    "\n",
    "\n",
    "    # Uses the DataQuality Operator to Run the DataQuality Checks across the listed tables\n",
    "\n",
    "    data_quality_checks = CapstoneDataQualityOperator(\n",
    "        task_id='i94_quality_checks',\n",
    "        dag=dag,\n",
    "        tables=('i94_visit_details_fact', 'state_demographics_dim', 'ethnicity_by_state_dim', 'temperatures_dim', 'us_airports_size_dim', 'location_codes_dim', 'travel_mode_dim', 'travel_purpose_dim', 'visa_type_codes_dim'),\n",
    "        dq_checks=default_args['query_checks'],\n",
    "        redshift_conn_id=\"redshift\"\n",
    "    )\n",
    "\n",
    "\n",
    "    # Ends the DAG Run\n",
    "\n",
    "    end_operator = DummyOperator(task_id='End_execution',  dag=dag)\n",
    "\n",
    "\n",
    "    # Sets the Order of Parallelization for DAG execution\n",
    "\n",
    "    start_operator >> import_i94_visit_details_fact\n",
    "    start_operator >> import_state_demographics_dim\n",
    "    start_operator >> import_ethnicity_by_state\n",
    "    start_operator >> import_temperatures_dim\n",
    "    start_operator >> import_us_airports_size_dim\n",
    "    start_operator >> import_location_codes_dim\n",
    "    start_operator >> import_travel_mode_dim\n",
    "    start_operator >> import_travel_purpose_dim\n",
    "    start_operator >> import_visa_type_codes_dim\n",
    "\n",
    "    import_i94_visit_details_fact >> data_quality_checks\n",
    "    import_state_demographics_dim >> data_quality_checks\n",
    "    import_ethnicity_by_state >> data_quality_checks\n",
    "    import_temperatures_dim >> data_quality_checks\n",
    "    import_us_airports_size_dim >> data_quality_checks\n",
    "    import_location_codes_dim >> data_quality_checks\n",
    "    import_travel_mode_dim >> data_quality_checks\n",
    "    import_travel_purpose_dim >> data_quality_checks\n",
    "    import_visa_type_codes_dim >> data_quality_checks\n",
    "\n",
    "    data_quality_checks >> end_operator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Content of \"capstone_csv_redshift.py\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "    from airflow.contrib.hooks.aws_hook import AwsHook\n",
    "    from airflow.hooks.postgres_hook import PostgresHook\n",
    "    from airflow.models import BaseOperator\n",
    "    from airflow.utils.decorators import apply_defaults\n",
    "\n",
    "    # Establishes the Class \"StageToRedshiftOperator\" within the DAG\n",
    "    # Designed to Copy Files from S3 Bucket to Staging Tables\n",
    "\n",
    "    class CsvToRedshiftOperator(BaseOperator):\n",
    "\n",
    "        ui_color = '#358140'\n",
    "\n",
    "        # Establishes the template SQL Copy Statements\n",
    "\n",
    "        copy_sql = \"\"\"\n",
    "            COPY {}\n",
    "            FROM '{}'\n",
    "            ACCESS_KEY_ID '{}'\n",
    "            SECRET_ACCESS_KEY '{}'\n",
    "            csv\n",
    "            IGNOREHEADER 1;\n",
    "        \"\"\"\n",
    "\n",
    "        # Defines default values from the Operator Call within the DAG, including context\n",
    "\n",
    "        @apply_defaults\n",
    "        def __init__(self,\n",
    "                     redshift_conn_id=\"\",\n",
    "                     aws_credentials_id=\"\",\n",
    "                     table=\"\",\n",
    "                     s3_bucket=\"\",\n",
    "                     # s3_key=\"\",\n",
    "                     json_path=\"auto\",\n",
    "                     data_path=\"\",\n",
    "                     delimiter=\",\",\n",
    "                     ignore_headers=1,\n",
    "                     *args, **kwargs):\n",
    "\n",
    "            super(CsvToRedshiftOperator, self).__init__(*args, **kwargs)\n",
    "            self.table = table\n",
    "            self.redshift_conn_id = redshift_conn_id\n",
    "            self.s3_bucket = s3_bucket\n",
    "            self.data_path = data_path\n",
    "            # self.s3_key = s3_key\n",
    "            self.aws_credentials_id = aws_credentials_id\n",
    "            self.json_path = json_path\n",
    "\n",
    "\n",
    "        # Function to Run Staging Tables Load \n",
    "\n",
    "        def execute(self, context):\n",
    "            aws_hook = AwsHook(self.aws_credentials_id)\n",
    "            credentials = aws_hook.get_credentials()\n",
    "            redshift = PostgresHook(postgres_conn_id=self.redshift_conn_id)\n",
    "\n",
    "            self.log.info(\"Clearing data from destination Redshift table\")\n",
    "            redshift.run(\"DELETE FROM {}\".format(self.table))\n",
    "\n",
    "            self.log.info(\"Copying data from S3 to Redshift\")\n",
    "            # rendered_key = self.s3_key.format(**context)\n",
    "            s3_path = \"s3://{}/{}\".format(self.s3_bucket, self.data_path)\n",
    "            formatted_sql = CsvToRedshiftOperator.copy_sql.format(\n",
    "                self.table,\n",
    "                s3_path,\n",
    "                credentials.access_key,\n",
    "                credentials.secret_key\n",
    "            )\n",
    "\n",
    "            redshift.run(formatted_sql)\n",
    "            self.log.info('Redshift Staging Tables Complete')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Content of \"capstone_json_redshift.py\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "    from airflow.contrib.hooks.aws_hook import AwsHook\n",
    "    from airflow.hooks.postgres_hook import PostgresHook\n",
    "    from airflow.models import BaseOperator\n",
    "    from airflow.utils.decorators import apply_defaults\n",
    "\n",
    "    # Establishes the Class \"StageToRedshiftOperator\" within the DAG\n",
    "    # Designed to Copy Files from S3 Bucket to Staging Tables\n",
    "\n",
    "    class JsonToRedshiftOperator(BaseOperator):\n",
    "\n",
    "        ui_color = '#358140'\n",
    "\n",
    "        # Establishes the template SQL Copy Statements\n",
    "\n",
    "        copy_sql = \"\"\"\n",
    "            COPY {}\n",
    "            FROM '{}'\n",
    "            ACCESS_KEY_ID '{}'\n",
    "            SECRET_ACCESS_KEY '{}'\n",
    "            FORMAT AS JSON '{}'\n",
    "            IGNOREHEADER 1\n",
    "            DATEFORMAT 'auto';\n",
    "        \"\"\"\n",
    "\n",
    "        # Defines default values from the Operator Call within the DAG, including context\n",
    "\n",
    "        @apply_defaults\n",
    "        def __init__(self,\n",
    "                     redshift_conn_id=\"\",\n",
    "                     aws_credentials_id=\"\",\n",
    "                     table=\"\",\n",
    "                     s3_bucket=\"\",\n",
    "                     # s3_key=\"\",\n",
    "                     json_path=\"auto\",\n",
    "                     data_path=\"\",\n",
    "                     delimiter=\",\",\n",
    "                     ignore_headers=1,\n",
    "                     *args, **kwargs):\n",
    "\n",
    "            super(JsonToRedshiftOperator, self).__init__(*args, **kwargs)\n",
    "            self.table = table\n",
    "            self.redshift_conn_id = redshift_conn_id\n",
    "            self.s3_bucket = s3_bucket\n",
    "            self.data_path = data_path\n",
    "            # self.s3_key = s3_key\n",
    "            self.aws_credentials_id = aws_credentials_id\n",
    "            self.json_path = json_path\n",
    "\n",
    "\n",
    "        # Function to Run Staging Tables Load \n",
    "\n",
    "        def execute(self, context):\n",
    "            aws_hook = AwsHook(self.aws_credentials_id)\n",
    "            credentials = aws_hook.get_credentials()\n",
    "            redshift = PostgresHook(postgres_conn_id=self.redshift_conn_id)\n",
    "\n",
    "            self.log.info(\"Clearing data from destination Redshift table\")\n",
    "            redshift.run(\"DELETE FROM {}\".format(self.table))\n",
    "\n",
    "            self.log.info(\"Copying data from S3 to Redshift\")\n",
    "            # rendered_key = self.s3_key.format(**context)\n",
    "            s3_path = \"s3://{}/{}\".format(self.s3_bucket, self.data_path)\n",
    "            formatted_sql = JsonToRedshiftOperator.copy_sql.format(\n",
    "                self.table,\n",
    "                s3_path,\n",
    "                credentials.access_key,\n",
    "                credentials.secret_key,\n",
    "                self.json_path\n",
    "            )\n",
    "\n",
    "            redshift.run(formatted_sql)\n",
    "            self.log.info('Redshift Staging Tables Complete')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### 4.2 Data Quality Checks\n",
    "To validate the processing of the Airflow DAG, the following validations were developed within the \"CapstoneDataQualityOperator\"\n",
    "- NULL Check to make sure that all records loaded properly\n",
    "- Record Length check to validate that all Redhsift Dimension Tables and Fact Table loaded properly\n",
    " \n",
    "The following is a screenshot of 4 completed runs for the DAG, including Data Quality Checks:\n",
    "![Completed Airflow Data Pipeline - 4 Runs](http://na-sjdemo1.marketo.com/rs/786-GZR-035/images/capstone_dag_completed_tree_view.png)\n",
    "\n",
    "> Link to Image of Complete Airflow Data Pipeline        \n",
    "> http://na-sjdemo1.marketo.com/rs/786-GZR-035/images/capstone_dag_completed_tree_view.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Content of \"capstone_data_quality.py\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "    import logging\n",
    "\n",
    "    from airflow.hooks.postgres_hook import PostgresHook\n",
    "    from airflow.models import BaseOperator\n",
    "    from airflow.utils.decorators import apply_defaults\n",
    "\n",
    "\n",
    "    # Establishes the Class \"DataQualityOperator\" within the DAG\n",
    "    # Designed to Run Data Quality Verifications on Capstone Analytics Tables\n",
    "\n",
    "    class CapstoneDataQualityOperator(BaseOperator):\n",
    "\n",
    "        ui_color = '#89DA59'\n",
    "\n",
    "\n",
    "        # Defines default values from the Operator Call within the DAG, including context\n",
    "\n",
    "        @apply_defaults\n",
    "        def __init__(self,\n",
    "                     redshift_conn_id=\"\",\n",
    "                     tables=[],\n",
    "                     dq_checks=[],\n",
    "                     *args, **kwargs):\n",
    "\n",
    "            super(CapstoneDataQualityOperator, self).__init__(*args, **kwargs)\n",
    "            self.tables = tables\n",
    "            self.dq_checks = dq_checks\n",
    "            self.redshift_conn_id = redshift_conn_id\n",
    "\n",
    "\n",
    "        # Function to Run Data Quality Checks - including NULL check and Record Count\n",
    "\n",
    "        def execute(self, context):\n",
    "            redshift = PostgresHook(postgres_conn_id=self.redshift_conn_id)\n",
    "\n",
    "            # NULL Check Evaluation Logic\n",
    "\n",
    "            for check in self.dq_checks:\n",
    "                sql = check.get('check_sql')\n",
    "                exp_result = check.get('expected_result')\n",
    "\n",
    "                records = redshift.get_records(sql)[0]\n",
    "\n",
    "                error_count = 0\n",
    "\n",
    "                if exp_result != records[0]:\n",
    "                    error_count += 1\n",
    "                    failing_tests.append(sql)\n",
    "\n",
    "                if error_count > 0:\n",
    "                    self.log.info('SQL Tests failed')\n",
    "                    self.log.info(failing_tests)\n",
    "                    raise ValueError('Data quality check failed')\n",
    "\n",
    "                if error_count == 0:\n",
    "                    self.log.info('SQL Tests Passed')\n",
    "\n",
    "            # Record Count Evaluation Logic\n",
    "\n",
    "            for table in self.tables:\n",
    "\n",
    "                records = redshift.get_records(f\"SELECT COUNT(*) FROM {table}\")\n",
    "\n",
    "                if len(records) < 1 or len(records[0]) < 1 or records[0][0] < 1:\n",
    "\n",
    "                    self.log.error(f\"Data quality check failed. {table} returned no results\")\n",
    "\n",
    "                    raise ValueError(f\"Data quality check failed. {table} returned no results\")\n",
    "\n",
    "                self.log.info(f\"Data quality on table {table} check passed with {records[0][0]} records\")\n",
    "\n",
    "            self.log.info('Data Quality Check Complete')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### 4.3 Data dictionary \n",
    "\n",
    "The following is a summary of the data that is contained within the i94 Analytics Tables, including a description of the data and where it came from."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.3.1 **\"i94_visit_details_fact\"**\n",
    "\n",
    "- This data comes from the US National Tourism and Trade Office and is a listing of visits to the United States and includes an appended Temp Id for Residence & Port.\n",
    "\n",
    "> Link to Immigration Data Source\n",
    "> https://travel.trade.gov/research/reports/i94/historical/2016.html\n",
    "\n",
    "\n",
    "| Field Name        |   Data Type  | Field Length |   Constraint   | Accepts Null? |                  Description                 |\n",
    "| :---              |     :----:   |    :----:    |     :----:     |    :----:     |                                         ---: |\n",
    "| i94rec            |    Integer   |      10      |   Primary Key  |      No       |  Visit Details Record ID                     |\n",
    "| i94_year          |    Integer   |      10      |       n/a      |      Yes      |  Visit Year                                  |\n",
    "| i94_month         |    Integer   |      10      |       n/a      |      Yes      |  Visit Month                                 |\n",
    "| i94_port_of_entry |    Varchar   |      30      |     Sort Key   |      Yes      |  Visit Entry Location                        |\n",
    "| i94_arrival_state |    Varchar   |      30      |       n/a      |      Yes      |  Visit Entry State                           |\n",
    "| i94_citizenship   |    Varchar   |      30      |       n/a      |      Yes      |  Visitor's Country of Citizenship            |\n",
    "| i94_residence     |    Varchar   |      30      |       n/a      |      Yes      |  Visitor's Country of Residence              |\n",
    "| arrival_mode      |    Integer   |      10      |       n/a      |      Yes      |  Visit Arrival Mode Code                     |\n",
    "| arrival_date      |    Numeric   |      10      |       n/a      |      Yes      |  Visit Arrival Date                          |\n",
    "| departure_date    |    Numeric   |      10      |       n/a      |      Yes      |  Visit Departure Date                        |\n",
    "| i94_age           |    Integer   |      10      |       n/a      |      Yes      |  Visitor's Age                               |\n",
    "| travel_purpose    |    Integer   |      10      |       n/a      |      Yes      |  Travel Purpose Code                         |\n",
    "| birth_year        |    Integer   |      10      |       n/a      |      Yes      |  Visitor's Birth Year                        |\n",
    "| gender            |    Varchar   |       2      |       n/a      |      Yes      |  Visitor's Gender                            |\n",
    "| visa_type         |    Varchar   |      10      |       n/a      |      Yes      |  Visitor's Visa Type                         |\n",
    "| residence_temp_id |    Integer   |      30      |       n/a      |      Yes      |  Id for Temperature at Country of Residence  |\n",
    "| port_temp_id      |    Integer   |      30      |       n/a      |      Yes      |  Id for Temperature at Port of Arrival       |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### **4.3.2 \"temperatures_dim\"**\n",
    "\n",
    "This dataset came from Kaggle and correlates temperatures of US Destination and Country of Origen by Month.\n",
    "\n",
    "> Link to World Temperature Data Source      \n",
    "> https://www.kaggle.com/berkeleyearth/climate-change-earth-surface-temperature-data\n",
    "\n",
    "\n",
    "| Field Name                |  Data Type   | Field Length |   Constraint   | Accepts Null? |                      Description                      |\n",
    "| :---                      |    :----:    |    :----:    |     :----:     |    :----:     |                                                  ---: |\n",
    "| temperature_id            |    Bigint    |      30      |   Primary Key  |      No       |  Temperatures Record ID                               |\n",
    "| temp_month                |    Integer   |      10      |      n/a       |      Yes      |  Temperature Record Month                             |\n",
    "| i94_state_code            |    Varchar   |      30      |      n/a       |      Yes      |  Location Code with Two Letters i.e. \"GA\"             |\n",
    "| i94_country_code          |    Varchar   |      30      |      n/a       |      Yes      |  Location Code for Country by Full Name i.e. \"Spain\"  |\n",
    "| temp_average              |    Integer   |      10      |      n/a       |      Yes      |  Average Temperature in Ferenheit                     |\n",
    "| stat_count                |    Integer   |      10      |      n/a       |      Yes      |  Count of Datapoints that create average value        |\n",
    "| temp_city                 |    Varchar   |      30      |      n/a       |      Yes      |  Temperature City Full Name i.e. \"Atlanta\"            |\n",
    "| temp_state                |    Varchar   |      30      |      n/a       |      Yes      |  Temperature State Abbreviation Used for Matching     |\n",
    "| temp_country              |    Varchar   |      30      |      n/a       |      Yes      |  Temperature Country Name Used for Matching           |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### **4.3.3 \"state_demographics_dim\" and \"ethnicity_by_state_dim\"**\n",
    "\n",
    "This data comes from OpenSoft and will be used to correlate US Destination Demographics by State.\n",
    "\n",
    "> Link to U.S. City Demographic Data      \n",
    "> https://public.opendatasoft.com/explore/dataset/us-cities-demographics/export/\n",
    "\n",
    "**\"state_demographics_dim\"\n",
    "\n",
    "| Field Name                |  Data Type   | Field Length |   Constraint   | Accepts Null? |                      Description                      |\n",
    "| :---                      |    :----:    |    :----:    |     :----:     |    :----:     |                                                  ---: |\n",
    "| i94_state_code            |    Varchar   |      10      |   Primary Key  |      No       |  Location Code ID for State, i.e. \"GA\"                |\n",
    "| median_age                |    Integer   |      10      |       n/a      |      Yes      |  Average Age for State                                |\n",
    "| male_population           |    Integer   |      20      |       n/a      |      Yes      |  Total of Male Population for State                   |\n",
    "| female_population         |    Integer   |      20      |       n/a      |      Yes      |  Total of Female Population for State                 |\n",
    "| total_population          |    Integer   |      20      |       n/a      |      Yes      |  Total Population for State                           |\n",
    "| number_of_veterans        |    Integer   |      20      |       n/a      |      Yes      |  Total Veterans within State                          |\n",
    "| foreign_born              |    Integer   |      20      |       n/a      |      Yes      |  Total of foreign born citizens within the State      |\n",
    "| avg_hh_size               |    Integer   |       4      |       n/a      |      Yes      |  Average Number of Members within a Household         |\n",
    "\n",
    "\n",
    "**\"ethnicity_by_state_dim\"**\n",
    "\n",
    "| Field Name                |  Data Type   | Field Length |   Constraint   | Accepts Null? |                      Description                      |\n",
    "| :---                      |    :----:    |    :----:    |     :----:     |    :----:     |                                                  ---: |\n",
    "| i94_state_code            |    Varchar   |      10      |   Primary Key  |      No       |  Location Code ID for State, i.e. \"GA\"                |\n",
    "| race                      |    Varchar   |      30      |       n/a      |      Yes      |  Ethnicity Category, i.e. \"Hispanic or Latino         |\n",
    "| state                     |    Varchar   |      10      |       n/a      |      Yes      |  State Id Used for Matching                           |\n",
    "| count                     |    Integer   |      30      |       n/a      |      Yes      |  Total of Population by Ethnicity                     |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### **4.3.4 \"us_airport_size_dim\"**\n",
    "\n",
    "This is a simple table of airport codes and corresponding cities the will be used to correlate Airport Size to US Destination.\n",
    "\n",
    "> Link to Airport Code Table      \n",
    "> https://datahub.io/core/airport-codes#data\n",
    "\n",
    "| Field Name                |  Data Type   | Field Length |   Constraint   | Accepts Null? |                      Description                      |\n",
    "| :---                      |    :----:    |    :----:    |     :----:     |    :----:     |                                                  ---: |\n",
    "| i94_port_code             |    Varchar   |      10      |   Primary Key  |      No       |  Location Code ID for City, i.e. \"ORL\"                |\n",
    "| state_code                |    Varchar   |      10      |       n/a      |      Yes      |  State Code Used for Matching                         |\n",
    "| type                      |    Varchar   |      10      |       n/a      |      Yes      |  Type of Ariport, i.e. \"Large, Medium or Small\"       |\n",
    "| city                      |    Varchar   |      30      |       n/a      |      Yes      |  Full City Name                                       |\n",
    "| ident                     |    Varchar   |      10      |       n/a      |      Yes      |  Airport Identifier for Future State Models           |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### **4.3.5 \"location_codes_dim\"**\n",
    "\n",
    "A table of aggregate Country & City Codes that related to the Location Fields within the i94 Immigration Data.\n",
    "\n",
    "> File Name for Location Codes: \"location_codes.csv\"\n",
    "\n",
    "| Field Name                |  Data Type   | Field Length |   Constraint   | Accepts Null? |                      Description                      |\n",
    "| :---                      |    :----:    |    :----:    |     :----:     |    :----:     |                                                  ---: |\n",
    "| location_code_id          |    Integer   |      30      |   Primary Key  |      No       |  Master Location Code                                 |\n",
    "| country_code              |    Varchar   |      30      |       n/a      |      Yes      |  Country Location Code                                |\n",
    "| country                   |    Varchar   |      30      |       n/a      |      Yes      |  Full Name of Country                                 |\n",
    "| state_code                |    Varchar   |      10      |       n/a      |      Yes      |  State Location Code                                  |\n",
    "| city                      |    Varchar   |      30      |       n/a      |      Yes      |  City Location Name                                   |\n",
    "| state                     |    Varchar   |      30      |       n/a      |      Yes      |  Full State Name                                      |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### **4.3.6 \"travel_mode_dim\"\n",
    "\n",
    "A table of travel codes that denote the transportation type used to reach the United States.\n",
    "\n",
    "> File Name for Travel Mode Codes: \"travel_mode_code.csv\"\n",
    "\n",
    "| Field Name                |  Data Type   | Field Length |   Constraint   | Accepts Null? |                      Description                      |\n",
    "| :---                      |    :----:    |    :----:    |     :----:     |    :----:     |                                                  ---: |\n",
    "| travel_mode_code          |    Varchar   |       2      |   Primary Key  |      No       |  Master Travel Mode Code                              |\n",
    "| mode                      |    Varchar   |      10      |       n/a      |      Yes      |  Travel Mode Detailed Description, i.e. \"Air\"         |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### **4.3.7 \"travel_purpose_dim\"\n",
    "\n",
    "A table of travel purpose codes that denotes the main reason for visiting the United States.\n",
    "\n",
    "> File Name for Travel Purpose Codes: \"travel_purpose.csv\"\n",
    "\n",
    "| Field Name                |  Data Type   | Field Length |   Constraint   | Accepts Null? |                      Description                      |\n",
    "| :---                      |    :----:    |    :----:    |     :----:     |    :----:     |                                                  ---: |\n",
    "| travel_purpose_code       |    Varchar   |       2      |   Primary Key  |      No       |  Master Travel Purpose Code                           |\n",
    "| travel_purpose            |    Varchar   |      10      |       n/a      |      Yes      |  Travel Purpose Detailed Description, i.e. \"Business\" |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### \"4.3.8 \"visa_type_codes_dim\"\n",
    "\n",
    "A table of Visa Type Codes that denotes the classification of the visit to the United States.\n",
    "\n",
    "> File Name for Visa Type Codes: \"visa_type_codes.csv\"\n",
    "\n",
    "| Field Name                |  Data Type   | Field Length |   Constraint   | Accepts Null? |                      Description                      |\n",
    "| :---                      |    :----:    |    :----:    |     :----:     |    :----:     |                                                  ---: |\n",
    "| visa_code                 |    Varchar   |       4      |   Primary Key  |      No       |  Master Visa Type Code                                |\n",
    "| visa_category             |    Varchar   |      10      |       n/a      |      Yes      |  Main Category for Visa Type, i.e. \"Travel Visa\"      |\n",
    "| visa_travel_purpose       |    Varchar   |      30      |       n/a      |      Yes      |  Detailed Description of Visa Type                    |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Step 5: Final Project Write Up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### 5.1 Rationale for Tools & Technologies within the Project\n",
    "\n",
    "The purpose of this project was to setup a repeatable, scalable process for analyzing i94 Travel Details to understand potential correlations to other data sets such as temperature, time of year, demographics of destination, and airport size. To support the simple, yet scalable use of data for analytics, a Fact & Dimensions Star Schema was developed to make it easy for teams using Redshift to access information that orginally was in multiple different formats.\n",
    "\n",
    "To make analytics simple, a central i94_travel_details Fact Table was produced that directly correlates to associated dimension tables through the use of primary and foreign keys. Additionally, pre-computed information such as a the sum of demographics and ethnicity were incorporated into the data model and ETL process to reduce the time required by analytics teams to wrangle data.\n",
    "\n",
    "Pyspark was chosen as for the technology foundation of the ETL based on the ability to handle large data sets and to create \"schema-on-read\", which helped with both the setup, configuration, and processing of the ETL. Amazon S3 was chosen as the storage location based on accessibility. Apache Airflow was selected for ingestion and validation of data into Redshift for use by the analytics team.\n",
    "\n",
    "The Pyspark ETL process was configured in a scalable way so that continued research into data quality and improved data sources could easily mirror the current setup with future additions or changes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### 5.2 Recommendations for Future Data Updates\n",
    "\n",
    "To build upon the current ETL Process, it is recommended that a monthly update of new i94 Travel Information be added to the current data set. Given the gaps within Location Tables information within the i94 Travel Details, it is recommended that an improved key be developed by US Immigration Services for Location information within the i94 core data set and potentially standardized to international encoding. Temperature Data would benefit from paid access to Temperature information from the listed sources to fill in gaps. Provided the considerations here, Temperature, Demographics, and Airport Data should be updated every 6 months to 1 year to aid in analytics accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### 5.3 Scaling Considerations for the Project\n",
    "Write a description of how you would approach the problem differently under the following scenarios:\n",
    "\n",
    "#### The data was increased by 100x.\n",
    "Provided a drastic increase in the volume of data, i.e. if the data were to be increased by 100x, the current process would still hold up, though it might be wise to move the ETL process to be controlled within Airflow as well. The current process for loading information from sources to Staging Buckets within Amazon S3 works from the full data set and Airflow would help to break up the ETL calls into batches by date, i.e. month or week. Additionally, the current processing for ETL through EMR uses an Cluster with 1 Core and 3 Supporting Clusters, depending on configuration of Airflow or the ETL, it is recommended that 1 Additional Supporting Cluster be added for every 2 Million Rows of i94 Data that are moved through the ETL.\n",
    "\n",
    "#### The data populates a dashboard that must be updated on a daily basis by 7am every day.\n",
    "If data were to be used in a realtime dashboard that updates on a daily basis by 7am every day, it is recommended that Airflow be used for both ETL Processing of new information and Setup of Redshift Analytics. Airflow processing would be moved from a monthly setting to a daily setting within the DAG. Given that the volume of information to be appended daily is relatively low, these systems could run nightly around 10pm to process data on non-peak hours through AWS and to ensure that any potential issues that arise could be addressed in the evening so that information is reliably available at 7am. Additionally, email alerts could be configured through Airflow to ensure that issues are known and addressable by the support team. \n",
    "\n",
    "#### The database needed to be accessed by 100+ people.\n",
    "If the database needs to be accessed by 100+ people, it would be important to setup preconfigured Redshift queries for users to help with less technical users and preventing non-power users from directly interfacing with the Redshift dataset. A structure would need to be put in place to handle questions, data quality issues, and escalations to ensure that users are consistently able access the information. It would be essential to have a dashboard that monitors data ingestion status, as well as a set of alerts that go out to Power Users of the Analytics Database should any errors or issues arrive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### X. Final Notes from the Project Author"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "It has been a real pleasure taking this class through Udacity, along with the assistance from the mentors. The knowledge learned within the class has delivered real-world skills and challenged students so that learnings can be applied with confidence and accuracy. Thank you to all of staff of Udacity for timely answers and helpful direction throughout the course!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
